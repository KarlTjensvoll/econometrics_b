{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b5806dc0e8395af2278a83b10faec683de35099ebc749f75874e48097d605efb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import NonLinearModels as nlm\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from numpy.random import default_rng\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "In this week's problem set, you are asked to investigate how alcohol\n",
    "abuse affects employment status. Individuals can either be outside of\n",
    "the labour market, unemployed or employed. This means, that the\n",
    "dependent variable has three discrete outcomes. We use the multinomial\n",
    "logit model to express the probability of individuals to be either\n",
    "outside the labour market, unemployed or employed. This model is a\n",
    "generalization of the logit model we looked at in Problem set 4 (binary\n",
    "choice) which only allowed for the dependent variable to have two\n",
    "discrete outcomes.\n",
    "\n",
    "In the multinomial logit the probability of observing choice $j$ for\n",
    "household $i$ given the observed explanatory variables, $x_{i}$ is given\n",
    "as,\n",
    "$$\n",
    "\\text{Pr}\\left(y_{i}=j\\mid x_{i},\\beta_j\\right)=p_{ij}=\\frac{\\exp\\left(x_{i}^{\\prime}\\beta_j\\right)}{\\sum_{m=1}^{J}\\exp\\left(x_{i}^{\\prime}\\beta_m\\right)},\\quad j=0,\\dots J \\tag{1}\n",
    "$$\n",
    "\n",
    "The probabilities lie between 0 and 1 since\n",
    "$\\exp\\left(x_{i}^{\\prime}\\beta_j\\right)>0$ and they sum to one over the\n",
    "J alternatives. The expression,\n",
    "(1), can be derived from an underlying latent\n",
    "utility model, where taste shocks follow an extreme value type 1\n",
    "distribution. The data-generating process of this latent utility model\n",
    "can be formalized as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{i} &= \\operatorname*{argmax}_{j=0,\\ldots,J} \\{u_{ij}\\}, &\n",
    "y_{i} &\\in \\{0,\\ldots,J\\}, & (2) \\\\[1ex] \n",
    "u_{ij} &= x_{i}'\\beta_j + \\varepsilon_{ij}, & \n",
    "\\varepsilon_{ij} &\\sim \\text{Gumbel}(0,1), & (3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the utility, $u_{ij}$, consists of a deterministic part,\n",
    "$V_{ij}=x_i'\\beta_j$, and a stochastic part, $\\epsilon_{ij}$. Individual\n",
    "$i$ chooses the alternative $y_i\\in \\{1,\\ldots,J\\}$ that is associated\n",
    "with the highest level of $u_{ij}$ after having observed the iid extreme\n",
    "value type 1 distributed shocks, $\\epsilon_{ij}$ for\n",
    "$j\\in\\{1,\\ldots,J\\}$. The choice probabilities\n",
    "(1) can be derived from this underlying utility\n",
    "model treating the utility shocks, $\\epsilon_{ij}$, as unobserved by the\n",
    "econometrician. The probability of individual $i$ choosing alternative\n",
    "$j$ is then equal to the probability that the utility, $u_{ij}$, is the\n",
    "highest among the $J$ alternatives that individual $i$ can choose\n",
    "between, i.e.,\n",
    "$p_{ij}=Pr(u_{ij}=\\max\\{u_{i0},\\ldots,u_{iJ}\\}\\mid x_i,\\beta_0,\\ldots,\\beta_J)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Question 1.\n",
    "-----------\n",
    "\n",
    "The first task is to finish the code in the function\n",
    "`sim_data(n, j, \\theta)` in the `NonLinearModels` module. This\n",
    "function simulates a data set with $n$ individuals choosing between $j$ alternatives. The third input is a parameter vector,\n",
    "$\\theta=(\\beta_1^\\prime,\\ldots,\\beta_J^\\prime)^\\prime$ and $J=0$ is the baseline alternative $\\beta_0=0$.\n",
    "\n",
    "The function simulates a $N \\times K$ matrix of covariates. These\n",
    "consist of a constant term and K-1 individual characteristics drawn from a standard normal distribution, i.e.\n",
    "$x_{i}= (1,x_{i2}, \\ldots, x_{iK})'$, with $x_{ik} \\sim N(0,1)$.\n",
    "\n",
    "The choice pattern is simulated in accordance with the data generating\n",
    "process given by the equations\n",
    "(2) - (3).\n",
    "\n",
    "*Note:*\n",
    "The solution to this is exactly as we simulated data for the clogit last week. The main difference is that we now have beta coefficients for each option (last week we had one column of beta coefficients, today we have (J - 1) columns of beta coefficients.). As mentioned, we use the first option J = 0 as a baseline.\n",
    "\n",
    "*Programming hing:*\n",
    "To practically implement this, you need to first initialize the `v` matrix from last time, for example like this: `v = np.zeros((n, j))`. Then you need to loop over the columns of `v`, except for the very first column, and fill these columns with the product from `x @ beta_j`, where `beta_j` is a column of the beta coefficients for that alternative. Again, since the first option is the baseline, `v` should end up with its first column filled with only zeros, and the rest of the columns should be filled with non-zero values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation parameters\n",
    "n = 10000\n",
    "j = 3\n",
    "\n",
    "# Initialize true beta coefficients for data generating process.\n",
    "beta2 = np.array([[-.3, .6]]).T\n",
    "beta3 = np.array([[.2, -1]]).T\n",
    "true_beta = np.hstack((beta2, beta3))\n",
    "theta0 = np.zeros((j - 1, j - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some data\n",
    "sim_result = nlm.sim_data(n, j, true_beta)\n",
    "y = sim_result.get('y')\n",
    "x = sim_result.get('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Category 0 has 2985 observations.\nCategory 1 has 2754 observations.\nCategory 2 has 4261 observations.\n"
     ]
    }
   ],
   "source": [
    "[print(f'Category {i} has {y[y == i].size} observations.') for i in np.unique(y)] ;"
   ]
  },
  {
   "source": [
    "Question 2. \n",
    "-----------\n",
    "\n",
    "The choice probabilities in the multinomial logit model is given by\n",
    "equation (1). This should be implemented into the mlogit.m\n",
    "class file such that the function\n",
    "`choice_prob()` returns choice probabilities, $p_{ij}$ for\n",
    "$i\\in \\{0,\\ldots,N\\}$ and $j\\in \\{0,\\ldots,N\\}$. The choice\n",
    "probabilities are contained in the ($N \\times J$) matrix `ccp`. The function should also return the (natural)\n",
    "$\\log$ of the choice probabilities which is used later in the maximum\n",
    "likelihood estimation. The log transformed choice probabilities, which should be saved in the matrix `logccp`,\n",
    "are given by\n",
    "$$\n",
    "\\log\\text{Pr}\\left(y_{i}=j\\mid x_{ij},\\beta\\right)=\\log p_{ij}=x_{i}^{\\prime}\\beta_j-\\log\\left(\\sum_{m=0}^{J}\\exp\\left(x_{i}^{\\prime}\\beta_m\\right)\\right),\n",
    "$$\n",
    "\n",
    "*Programming hint*:\n",
    "Again, this is done exactly like last week, except that you need a loop for each column of coefficients we have for the choices when you calculate the utility matrix. Remember that the first column in the utility matrix is the baseline, and therefore should be a column with only zero values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp, logccp = nlm.choice_prob(theta0, x)"
   ]
  },
  {
   "source": [
    "Question 3. \n",
    "-----------\n",
    "\n",
    "Last time you created the criterion function for the conditional logit estimator. This is identical to the same criterion function for the multinomial logit model, so you need only to copy this from last week.\n",
    "\n",
    "Now estimate the coefficients based on the simulated data, using maximum likelihood to solve the model. To do this use the `nlm.estimate()` function.\n",
    "\n",
    "I have written some code that calculates the norm of the gradient at the point of the solution. We will use this later to solve any issues we might have in a real, empirical example. Should this norm be large or small?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.930590\n         Iterations: 17\n         Function evaluations: 90\n         Gradient evaluations: 18\n"
     ]
    }
   ],
   "source": [
    "sim_result = nlm.estimate(nlm.mlogit, theta0, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.3  0.2]\n [ 0.6 -1. ]]\nResults\nDependent variable: y\n\n          beta         se\n---  ---------  ---------\nx21  -0.349287  0.0308072\nx22   0.177073  0.0262268\nx31   0.641211  0.0331805\nx32  -1.00197   0.0318855\nIn 17 iterations and 90 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "print(true_beta)\n",
    "nlm.print_table(('y', ['x21', 'x22', 'x31', 'x32']), sim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_norm(q, theta):\n",
    "    print(np.linalg.norm(\n",
    "        np.sum(nlm.centered_grad(q, theta), axis=0)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.944840957579523e-06\n"
     ]
    }
   ],
   "source": [
    "q = lambda theta: -(1/n)*np.sum(nlm.mlogit(theta, y, x))\n",
    "calc_norm(q, sim_result['b_hat'])"
   ]
  },
  {
   "source": [
    "Question 4. \n",
    "-----------\n",
    "\n",
    "We are now going to use the data from Terza (2002) aticle, ALCOHOL ABUSE AND EMPLOYMENT: A SECOND LOOK, and estimate the\n",
    "multinomial logit model. Therefore, we need to first load the data and\n",
    "then use the proper functions coded previously. \n",
    "\n",
    "The persons in this data have three states: 0 if they are out of the work force, 1 if they are unemployed, and 2 if they are employed. Out of the work force will be the baseline for our analysis.\n",
    "\n",
    "The results we are looking to obtain are shown in the table below, although some of them are rescaled, so you might have too many commas for some coefficients:\n",
    "\n",
    "``` {.matlab}\n",
    "                      Unemployed             Employed              \n",
    "                             \n",
    "   Alc. abuse:    0.127    [t =   0.59]   -0.153    [t =  -1.10]\n",
    "   Unemp.rate:    0.458    [t =   0.92]   -0.955    [t =  -2.85]\n",
    "          Age:    1.618    [t =   2.41]    2.272    [t =   5.49]\n",
    "  Age-squared:   -2.438    [t =  -2.99]   -3.080    [t =  -6.29]\n",
    "    Schooling:   -0.092    [t =  -0.39]    0.891    [t =   5.55]\n",
    "      Married:    0.400    [t =   1.96]    0.709    [t =   5.39]\n",
    "  Family size:    0.062    [t =   1.20]    0.062    [t =   1.75]\n",
    "        White:    0.039    [t =   0.23]    0.738    [t =   6.47]\n",
    "    Excellent:    2.918    [t =   6.41]    3.703    [t =  19.41]\n",
    "    Very Good:    2.978    [t =   6.52]    3.653    [t =  18.87]\n",
    "         Good:    2.494    [t =   5.53]    3.000    [t =  16.16]\n",
    "         Fair:    1.460    [t =   2.99]    1.876    [t =   9.57]\n",
    "    Northwest:    0.085    [t =   0.36]    0.089    [t =   0.59]\n",
    "      Midwest:    0.016    [t =   0.08]    0.123    [t =   0.95]\n",
    "        South:    0.175    [t =   0.87]    0.439    [t =   3.28]\n",
    "  Center City:   -0.272    [t =  -1.42]   -0.269    [t =  -2.12]\n",
    "    Other MSA:   -0.092    [t =  -0.47]    0.098    [t =   0.77]\n",
    "  1st quarter:    0.422    [t =   2.14]   -0.028    [t =  -0.21]\n",
    "  2nd quarter:   -0.022    [t =  -0.11]   -0.111    [t =  -0.88]\n",
    "  3rd quarter:   -0.037    [t =  -0.17]   -0.053    [t =  -0.40]\n",
    "     Constant:   -6.114    [t =  -4.31]   -6.237    [t =  -7.17]\n",
    "     \n",
    "     Value of the log-likehood function: -3217.48\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I have written the code to load the data. Use this data to estimate a multinomial logit model using the `nlm.estimate()` function, and print it out using the `nlm.print_table()` function.\n",
    "\n",
    "You might get an error that states that the minimizer is unsure whether it found a solution. Reuse the code from above and check the size of the norm at the solution. Remember that you have to redefine `q` as well. Does the norm look fine to you?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('terza.txt')\n",
    "y = data[:, 0] - 1  # Subtract 1 to make into 0-index\n",
    "y = y.astype(int)\n",
    "\n",
    "const = np.ones((y.size, 1))\n",
    "x = np.hstack((data[:, 1:], const))\n",
    "\n",
    "y_lab = 'Employed - Unemployed'\n",
    "x_lab = ['alcohol', 'urate', 'age', 'agesq', 'schooling', 'married', 'famsize', 'white', 'excellent', 'verygood', 'good', 'fair', 'northeast', 'midwest', 'south', 'center', 'other', 'firstq', 'secondq', 'thirdq', 'const']\n",
    "\n",
    "j = int(y.max())  # Number of alternatives, including 0.\n",
    "\n",
    "theta0 = np.zeros((x.shape[1], j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nlm.estimate and nlm.print_table to estimate the model and print it out nicely. You should get coefficients that are close to the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 0.327579\n         Iterations: 285\n         Function evaluations: 14288\n         Gradient evaluations: 332\n"
     ]
    }
   ],
   "source": [
    "mlogit_result = nlm.estimate(nlm.mlogit, theta0, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results\nDependent variable: Employed - Unemployed\n\n             beta      se    beta      se\n---------  ------  ------  ------  ------\nalcohol     0.127  0.2173  -0.154  0.1413\nurate       0.046  0.0545  -0.095  0.0343\nage         0.161  0.0673   0.227  0.0418\nagesq      -0.002  0.0008  -0.003  0.0005\nschooling  -0.009  0.0271   0.089  0.0147\nmarried     0.401  0.1875   0.709  0.1147\nfamsize     0.062  0.0500   0.062  0.0317\nwhite       0.039  0.1768   0.738  0.1044\nexcellent   2.918  0.4551   3.703  0.1813\nverygood    2.978  0.4599   3.653  0.1873\ngood        2.494  0.4528   2.999  0.1735\nfair        1.460  0.4890   1.876  0.1824\nnortheast   0.084  0.2412   0.089  0.1497\nmidwest     0.015  0.2097   0.123  0.1301\nsouth       0.175  0.2080   0.439  0.1274\ncenter     -0.272  0.1964  -0.269  0.1216\nother      -0.092  0.1944   0.098  0.1253\nfirstq      0.423  0.2031  -0.027  0.1300\nsecondq    -0.021  0.2136  -0.111  0.1273\nthirdq     -0.036  0.2173  -0.053  0.1285\nconst      -6.099  1.4862  -6.226  0.9375\nIn 285 iterations and 14288 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "nlm.print_table(\n",
    "    (y_lab, x_lab), \n",
    "    mlogit_result, \n",
    "    floatfmt=['', '.3f', '.4f', '.3f', '.4f']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of the gradient, you can reuse the function defined above, but you need to redefine the anonymus function q yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22.48437868687972\n"
     ]
    }
   ],
   "source": [
    "q = lambda theta: np.sum(nlm.mlogit(theta, y, x))\n",
    "calc_norm(q, mlogit_result['b_hat'])"
   ]
  },
  {
   "source": [
    "Question 5. \n",
    "-----------\n",
    "If the magnitudes of our covariates vary by a lot, our minimizer may sometimes have issues finding a solution to our problem, even though it is very close to a solution.\n",
    "\n",
    "I have made a copy of `x`, called `x_scale`. Downscale columns 1, 2 and 4 in `x_scale` by 10, and downscale column 3 by 1000. (By downscale I mean that you divide the values by 10 or 1000). Then resolve the model with this rescaled `x` matrix. Is the model now solved? Do the results differ? How does the norm look like now?\n",
    "\n",
    "To reitterate:\n",
    "- Rescale the `x` matrix into the new `x_scale` matrix\n",
    "- Resolve the model using the new `x_scale` matrix\n",
    "- Print out a table with the new results, do they differ from the previous solution?\n",
    "- Calculate the norm of the new solution, does it look better?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale = x.copy()\n",
    "x_scale[:, 1] = x_scale[:, 1]/10\n",
    "x_scale[:, 2] = x_scale[:, 2]/10\n",
    "x_scale[:, 3] = x_scale[:, 3]/1000\n",
    "x_scale[:, 4] = x_scale[:, 4]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat what you did in the previous question. Does the norm look better now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.327579\n         Iterations: 305\n         Function evaluations: 13158\n         Gradient evaluations: 306\n"
     ]
    }
   ],
   "source": [
    "mlogit_result2 = nlm.estimate(nlm.mlogit, mlogit_result['b_hat'], y, x_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results\nDependent variable: Employed - Unemployed\n\n              beta      se     beta      se\n---------  -------  ------  -------  ------\nalcohol     0.1234  0.2173  -0.1553  0.1413\nurate       0.4573  0.5449  -0.9539  0.3433\nage         1.6187  0.6727   2.2629  0.4181\nagesq      -2.4402  0.8086  -3.0696  0.4872\nschooling  -0.0961  0.2713   0.8884  0.1473\nmarried     0.3981  0.1875   0.7078  0.1147\nfamsize     0.0622  0.0500   0.0622  0.0317\nwhite       0.0397  0.1768   0.7379  0.1044\nexcellent   2.9071  0.4551   3.7027  0.1814\nverygood    2.9663  0.4599   3.6527  0.1873\ngood        2.4824  0.4528   2.9991  0.1735\nfair        1.4507  0.4890   1.8764  0.1824\nnortheast   0.0879  0.2412   0.0903  0.1497\nmidwest     0.0180  0.2097   0.1236  0.1301\nsouth       0.1774  0.2080   0.4398  0.1274\ncenter     -0.2711  0.1964  -0.2690  0.1216\nother      -0.0904  0.1944   0.0985  0.1253\nfirstq      0.4219  0.2031  -0.0258  0.1300\nsecondq    -0.0226  0.2136  -0.1092  0.1273\nthirdq     -0.0364  0.2173  -0.0511  0.1285\nconst      -6.0974  1.4862  -6.2183  0.9376\nIn 305 iterations and 13158 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "nlm.print_table(\n",
    "    (y_lab, x_lab), \n",
    "    mlogit_result2, \n",
    "    floatfmt=['', '.4f', '.4f', '.4f', '.4f']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.22933895797962423\n"
     ]
    }
   ],
   "source": [
    "q = lambda theta: np.sum(nlm.mlogit(theta, y, x_scale))\n",
    "calc_norm(q, mlogit_result2['b_hat'])"
   ]
  },
  {
   "source": [
    "## Question 6.\n",
    "\n",
    "Consider a particular individual represented by\n",
    "the row vector of covariates\n",
    "$x_0 = (0, 6, 40, 40^2, 8, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1)$\n",
    "where the first element indicates that the individual does not abuse\n",
    "alcohol ($x_0[1]= 0$). Note that to facilitate convergence we need to\n",
    "again rescale some of the covariates. Following the rescale proposed in\n",
    "the expost code, the vector of covariates will become\n",
    "$x_0 = (0, 0.6, 4, 1.6, 0.8, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1)$.\n",
    "\n",
    "The odds ratio measures the probability of a given outcome to\n",
    "occur relative to the probability of a baseline outcome, conditional on\n",
    "the covariates. It is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{Pr(y_i=j|x_i)}{Pr(y_i=1|x_i)} = \\frac{\\frac{\\exp \\{ x_{i}^{\\prime}\\beta_j \\}}{ \\sum_{l=1}^J \\exp \\{ x_{i}^{\\prime}\\beta_l \\}}}   {\\frac{\\exp \\{ x_{i}^{\\prime}\\beta_1 \\}}{ \\sum_{l=1}^J \\exp \\{ x_{i}^{\\prime}\\beta_l \\}}} = \\exp\\{x_i^\\prime \\beta_j\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The odds ratio of being employed is therefore calculated as\n",
    "$x_0^\\prime \\beta_3$, where $\\beta_3$ is the vector of parameters\n",
    "estimated for the third outcome (being employed).\n",
    "\n",
    "- Calculate the odds ratio of being unemployed to be around 0.593.\n",
    "- Calculate the odds ratio of being employed to be around 5.742.\n",
    "\n",
    "How do we interpret these odds ratios?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider some person\n",
    "x0 = np.array([[0, 6, 40, 40*40, 8, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1]], dtype=np.float).reshape(1, -1)\n",
    "# Rescale\n",
    "x0[:, 1] = x0[:, 1]/10\n",
    "x0[:, 2] = x0[:, 2]/10\n",
    "x0[:, 3] = x0[:, 3]/1000\n",
    "x0[:, 4] = x0[:, 4]/10\n",
    "\n",
    "x1 = x0.copy()\n",
    "x1[0, 0] = 1.0\n",
    "betahat = mlogit_result2['b_hat'].reshape(21, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-2918f6d0b3d1>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-2918f6d0b3d1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    odds_unemp =  # FILL IN\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Calculate the odds ratio of being unemployed/employed for the person x0. These odds ratoi are compared to the baseline out of the workforce. How would you interpret these coefficients?\n",
    "odds_unemp =  # FILL IN\n",
    "odds_emp =  # FILL IN\n",
    "print(f'{odds_unemp.item():.3f}')\n",
    "print(f'{odds_emp.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.598\n5.758\n"
     ]
    }
   ],
   "source": [
    "# odds ratio of being unemployed/employed vs being out of labor force\n",
    "odds_unemp = np.exp(x0 @ betahat[:, 0])\n",
    "odds_emp = np.exp(x0 @ betahat[:, 1])\n",
    "print(f'{odds_unemp.item():.3f}')\n",
    "print(f'{odds_emp.item():.3f}')"
   ]
  },
  {
   "source": [
    "Question 7. \n",
    "-----------\n",
    "\n",
    "The marginal effect of a change in a binary covariate is computed by\n",
    "subtracting the conditional choice probability under the two possible\n",
    "values of the binary covariate $k$ (with and without alcohol\n",
    "abuse): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^d_j(x_{ik})=Pr(y_i=j|x_{ik}=1)-Pr(y_i=j|x_{ik}=0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Cacluate the choice probability for person $x0$.\n",
    "- Cacluate the choice probability for the same person but now with alcholoh abuse set to 1 (x1).\n",
    "- Subtrace the choice probabilty of x1 from x0.\n",
    "- You should get values close to `[ 0.0154  0.0211  -0.0365]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marg_effect_discrete(betahat, x0, x1):\n",
    "    # Calculate the probabilities of x1 and x0, using betahat of the lates model estimation and nlm.choice_prob (remember, it returns two outputs, but we are not interested in the log probabilites, so we often use \"_\" for something we are not going to use).\n",
    "    # Then substract the probability vector of x0 from the probability vector of x1.\n",
    "    prob_x1, _ =  # FILL IN\n",
    "    prob_x0, _ =  # FILL IN\n",
    "    return (prob_x1 - prob_x0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marg_effect_discrete(betahat, x0, x1):\n",
    "    # Calculate the probabilities of x1 and x0, using betahat of the lates model estimation.\n",
    "    # Then substract the probability vector of x0 from the probability vector of x1.\n",
    "    prob_x1, _ = nlm.choice_prob(betahat, x1)\n",
    "    prob_x0, _ = nlm.choice_prob(betahat, x0)\n",
    "    return (prob_x1 - prob_x0).flatten()\n",
    "\n",
    "def marg_effect_continous(betahat, betahat_subset, x):\n",
    "    # Use betahat to calculate the probability vector for person x0.\n",
    "    # Then use the formula given above to calculate the marginal effect, using the probability vector and the betahat_subset.\n",
    "    prob, _ = nlm.choice_prob(betahat, x)\n",
    "    return (prob.T*(betahat_subset - prob @ betahat_subset)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.01542498  0.0211127  -0.03653768]\n"
     ]
    }
   ],
   "source": [
    "me_alc = marg_effect_discrete(mlogit_result2['b_hat'], x0, x1)\n",
    "print(me_alc)"
   ]
  },
  {
   "source": [
    "## Question 8.\n",
    "\n",
    "The marginal effect for the multinomial logit model with respect to a\n",
    "continuous covariate $k$ (e.g., schooling) is defined as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial Pr(y_i=j\\mid x_i)}{\\partial x_{ik}} = p_{ij} \\left(\\beta_{jk} - \\sum_{l=1}^J p_{il}\\beta_{lk}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_{ij}$ is the $1 \\times 3$ row vector of conditional choice probabilites for our representative person $x_0$, and $\\beta_{jk}$ is a $3 \\times 1$ vector of the beta coefficients for each choice (remember that the first choice is the baseline, and its coefficient is therefore 0). Note that $\\sum_{l=1}^J p_{il}\\beta_{lk}$ can be calculated as the dot product between $p_{ij}$ and $\\beta_{jk}$.\n",
    "\n",
    "- Calculate the marginal effect of education using the above equation. $\\beta_{jk}$ is given to you.\n",
    "- You should get values close to `[-0.00940 -0.00632  0.0157]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marg_effect_continous(betahat, betahat_subset, x):\n",
    "    # Use betahat to calculate the probability vector for person x0.\n",
    "    # Then use the formula given above to calculate the marginal effect, using the probability vector and the betahat_subset.\n",
    "    prob, _ = nlm.choice_prob(betahat, x)\n",
    "    return  # FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.00934674 -0.00637133  0.01571807]\n"
     ]
    }
   ],
   "source": [
    "betahat_educ = np.array(\n",
    "    [[0.0, betahat[4, 0] / 10, betahat[4, 1] / 10]]\n",
    ").T\n",
    "me_educ = marg_effect_continous(betahat, betahat_educ, x0)\n",
    "print(me_educ)"
   ]
  },
  {
   "source": [
    "## Bootstrapping to get standard errors of marginal effects.\n",
    "\n",
    "Since we are not able to directly calculate variances for the marginal effects, we can use bootstrapping instead. This is done by taking a sample of our data, but with replacement.\n",
    "\n",
    "This means that we create a new data set as a random draw form our original data set, but we allow that the same observation is drawn multiple times. This new data set has the same number of observations as the original, but will differ sligthly since we now have some duplicates.\n",
    "\n",
    "We then perform our marginal effect calculations again on this new sample, save the results, and then draw a new random sample from the data (again, with replacement). We continue this process a number of times, and then calculate the standard deviation of the stored marginal effects. This standard deviation is what we will use as standard deviations to the marginal effects calculated on the original data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nboot = 10  # Number of bootstraps\n",
    "\n",
    "# Initialize matrices to store bootstrap results\n",
    "me_alc_boot = np.zeros((nboot, 3))\n",
    "me_educ_boot = np.zeros((nboot, 3))\n",
    "\n",
    "# Set seed for random sampling.\n",
    "seed = 42\n",
    "rng = default_rng()\n",
    "\n",
    "# Stack y and x together, in order to make sure we make a random draw of the same row.\n",
    "data = np.hstack((y.reshape(-1, 1), x_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(data):\n",
    "    \"\"\"Creates a new data set using random draws of the original data set, allowing for replacement. In other words, we allow the same row to be drawn multiple times.\n",
    "    \"\"\"\n",
    "    data_sample = rng.choice(data, n)  # Makes n random draws from data, allowing replacement.\n",
    "\n",
    "    # We then separate out back to the y vector and x matrix.\n",
    "    y_sample = data_sample[:, 0]\n",
    "    y_sample = y_sample.astype(int)  # The y vector is recast into a float vector during the previous np.hstack, but we need it to be int for indexing later. We therefore recast it back into int.\n",
    "    x_sample = data_sample[:, 1:]\n",
    "    return y_sample, x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bootstrap iteration 0\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.326028\n",
      "         Iterations: 213\n",
      "         Function evaluations: 9202\n",
      "         Gradient evaluations: 214\n",
      "Bootstrap iteration 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.326873\n",
      "         Iterations: 287\n",
      "         Function evaluations: 12384\n",
      "         Gradient evaluations: 288\n",
      "Bootstrap iteration 2\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.325688\n",
      "         Iterations: 270\n",
      "         Function evaluations: 11696\n",
      "         Gradient evaluations: 272\n",
      "Bootstrap iteration 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.331462\n",
      "         Iterations: 278\n",
      "         Function evaluations: 12040\n",
      "         Gradient evaluations: 280\n",
      "Bootstrap iteration 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.336972\n",
      "         Iterations: 294\n",
      "         Function evaluations: 12728\n",
      "         Gradient evaluations: 296\n",
      "Bootstrap iteration 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.322494\n",
      "         Iterations: 326\n",
      "         Function evaluations: 14061\n",
      "         Gradient evaluations: 327\n",
      "Bootstrap iteration 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.327346\n",
      "         Iterations: 227\n",
      "         Function evaluations: 9847\n",
      "         Gradient evaluations: 229\n",
      "Bootstrap iteration 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.318153\n",
      "         Iterations: 291\n",
      "         Function evaluations: 12599\n",
      "         Gradient evaluations: 293\n",
      "Bootstrap iteration 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.316831\n",
      "         Iterations: 288\n",
      "         Function evaluations: 12427\n",
      "         Gradient evaluations: 289\n",
      "Bootstrap iteration 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.345749\n",
      "         Iterations: 240\n",
      "         Function evaluations: 10406\n",
      "         Gradient evaluations: 242\n"
     ]
    }
   ],
   "source": [
    "for i in range(nboot):\n",
    "    print(f'Bootstrap iteration {i}')\n",
    "    k = x.shape[1]\n",
    "\n",
    "    # Get a new random sample each iteration, and resolve the model.\n",
    "    y_boot, x_boot = random_sample(data)\n",
    "    boot_result = nlm.estimate(\n",
    "        nlm.mlogit, \n",
    "        mlogit_result2['b_hat'], \n",
    "        y_boot, \n",
    "        x_boot\n",
    "    )\n",
    "\n",
    "    # Save the estimated coefficients, and make the ready for calculating new marginal effects\n",
    "    betahat_boot = boot_result['b_hat']\n",
    "    betahat_boot = betahat_boot.reshape(k, -1)\n",
    "    betahat_boot_educ = np.array(\n",
    "        [[0.0, betahat_boot[4, 0] / 10, betahat_boot[4, 1] / 10]]\n",
    "    ).T  # This is the education coefficients.\n",
    "\n",
    "    # Calculate the marginal effects, and store them for later use.\n",
    "    me_alc_boot[i] = marg_effect_discrete(betahat_boot, x0, x1)\n",
    "    me_educ_boot[i] = marg_effect_continous(betahat_boot, betahat_boot_educ, x0)"
   ]
  },
  {
   "source": [
    "Here is a function to make a semi-nice table to print the coefficients, and their 95% confidence interval."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def me_table(me_coeff, me_se, **kwargs):\n",
    "    table = np.column_stack(\n",
    "        (me_coeff, me_coeff -1.96*me_se, me_coeff +1.96*me_se)\n",
    "    )\n",
    "    print(tabulate(table, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Marginal effect of alcohol on being out of workforce, unemployment or employment:\n  Coeff      0.5    0.95\n-------  -------  ------\n 0.0154  -0.0151  0.0460\n 0.0211  -0.0075  0.0498\n-0.0365  -0.0744  0.0013\n"
     ]
    }
   ],
   "source": [
    "print(\"Marginal effect of alcohol on being out of workforce, unemployment or employment:\")\n",
    "me_table(\n",
    "    me_alc, \n",
    "    np.std(me_alc_boot, axis=0),\n",
    "    headers=['Coeff', '0.5', '0.95'], \n",
    "    floatfmt='.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Marginal effect of education on being out of workforce, unemployment or employment:\n  Coeff      0.5     0.95\n-------  -------  -------\n-0.0093  -0.0161  -0.0026\n-0.0064  -0.0114  -0.0014\n 0.0157   0.0077   0.0237\n"
     ]
    }
   ],
   "source": [
    "print(\"Marginal effect of education on being out of workforce, unemployment or employment:\")\n",
    "me_table(\n",
    "    me_educ, \n",
    "    np.std(me_educ_boot, axis=0),\n",
    "    headers=['Coeff', '0.5', '0.95'], \n",
    "    floatfmt='.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}