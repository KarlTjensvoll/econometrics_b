{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from numpy import random\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import genextreme\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "from tabulate import tabulate\n",
    "import NonLinearModels_ante as nlm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, you will replicate part of the results in\n",
    "Brownstone and Train (1999). You will estimate the conditional logit\n",
    "model given the available data.\n",
    "\n",
    "The Conditional Logit (CL) Model\n",
    "================================\n",
    "\n",
    "The Conditional Logit (CL) model is a powerful tool for analyzing\n",
    "descrete choices, which is heavily used in e.g. empirical industrial\n",
    "organization. In this model, decision makers have to choose between $J$\n",
    "different discrete options, $\\{1,2,...,J\\}$. An option might be a car.\n",
    "For each option, we observe a vector of characteristics,\n",
    "$x_{j}\\in\\mathbb{R}^{K}$, and for each individual $i=1,...,N$, we\n",
    "observe the chosen alternative, $y_{i}\\in\\{1,...,J\\}$. If individuals\n",
    "face different alternatives, e.g. if the prices or characteristics of\n",
    "cars available were different, then characteristics also vary across\n",
    "individuals, $x_{ij}\\in\\mathbb{R}^{K}$.\n",
    "\n",
    "Our model assumes that individual $i$ chose the alternative that\n",
    "maximized utility, \n",
    "\n",
    "$$y_{i}=\\arg\\max_{j\\in\\{1,...,J\\}}u_{ij}.$$ \n",
    "\n",
    "Our model for utility takes the form\n",
    "\n",
    "$$u_{ij}=x_{ij}\\beta+\\varepsilon_{ij},\\quad\\varepsilon_{ij}\\sim\\text{IID Extreme Value Type I}.$$\n",
    "\n",
    "That is, utility is composed of a part that depends on observables,\n",
    "$x_{ij}\\beta$, and an idiosyncratic error term, $\\varepsilon_{ij}$,\n",
    "observed to the individual but not to the econometrician. The problem of\n",
    "estimation is to recover $\\beta$ without knowledge on\n",
    "$\\varepsilon_{ij}$.\n",
    "\n",
    "It turns out that the distributional form for $\\varepsilon_{ij}$ implies\n",
    "that\n",
    "\n",
    "$$\n",
    "\\Pr(y_{i}=j|\\mathbf{X}_{i})=\\frac{\\exp(x_{ij}\\beta)}{\\sum_{k=1}^{J}\\exp(x_{ik}\\beta)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}_{i}=(x_{i1},x_{i2},...,x_{iJ})$. This remarkable\n",
    "result was first introduced to economists by nobel laureate Daniel\n",
    "McFadden. Taking logarithms, we obtain a particularly parsimonious form\n",
    "for the log-likelihood contribution:\n",
    "\n",
    "$$\\ell_{i}(\\beta)=x_{ij}\\beta-\\log\\sum_{k=1}^{J}\\exp(x_{ik}\\beta).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max rescaling for numerical stability\n",
    "-------------------------------------\n",
    "\n",
    "A particularly important numerical trick that one is typically forced to\n",
    "use with logit models is what is called *max rescaling*. For simplicity,\n",
    "let $v_{ij}\\equiv x_{ij}'\\beta$. Now, note that for any\n",
    "$K_{i}\\in\\mathbb{R}$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\exp(v_{ij})}{\\sum_{k=1}^{J}\\exp(v_{ik})} & = \\frac{\\exp(v_{ij})}{\\sum_{k=1}^{J}\\exp(v_{ik})}\\frac{\\exp(-K_{i})}{\\exp(-K_{i})}\\\\\n",
    " & = \\frac{\\exp(v_{ij}-K_{i})}{\\sum_{k=1}^{J}\\exp(v_{ik}-K_{i})}.\n",
    " \\end{aligned}\n",
    " $$\n",
    "\n",
    "This means that we can subtract any scalar from all utilities for an\n",
    "individual. This is very useful because the exponential function is a\n",
    "highly unstable numerical object on any computer: for large or small\n",
    "values of $z$, $\\exp(z)$ will result in round-up or round-down errors,\n",
    "respectively. Since round-up errors are particularly bad for estimation,\n",
    "it turns out to be useful to choose $K_{i}$ so that we avoid them, even\n",
    "at the cost of encountering more round-down errors. Thus, we choose\n",
    "\n",
    "$$\n",
    "K_{i}=\\max_{j\\in\\{1,...,J\\}}v_{ij},\n",
    "$$ \n",
    "\n",
    "and subtract $K_{i}$ from all utilities before taking any exponential values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compensating variation in logit models\n",
    "--------------------------------------\n",
    "\n",
    "A useful feature of logit models is that they provide a neat welfare\n",
    "measure in the form of what is commonly referred to as the \"log sum.\"\n",
    "This is because of the fact that\n",
    "\n",
    "$$\\mathbb{E}_{\\varepsilon_{i1},...,\\varepsilon_{iJ}}\\left[\\max(v_{ij}+\\varepsilon_{ij})\\right]=\\log\\left[\\sum_{j=1}^{J}\\exp(v_{ij})\\right].$$\n",
    "\n",
    "Because the left-hand side is the expected utility, prior to knowing the\n",
    "error terms, of the choice instance, it can be thought of the \"value\" of\n",
    "the choice instance. If one of the variables, say the first, is a price\n",
    "variable, then $\\beta_{1}$ is the marginal utility of price, converting\n",
    "money into utils. Thus, economists tend to divide the welfare measure\n",
    "with $\\beta_{1}$ to get a money-metric utility measure. Again, the level\n",
    "of that measure in itself may not be useful, but differences are.\n",
    "Suppose we change something about the utilities, from $v_{ij}$ to\n",
    "$\\tilde{v}_{ij}$, and want to compute the compensating variation: that\n",
    "is, how much we would have to pay the agent (regardless of the chosen\n",
    "alternative) to make the agent indifferent between being placed in the\n",
    "first or the second choice instance. We can compute that as\n",
    "\n",
    "$$CV=\\frac{1}{\\beta_{1}}\\log\\left[\\sum_{j=1}^{J}\\exp(\\tilde{v}_{ij})\\right]-\\frac{1}{\\beta}\\log\\left[\\sum_{j=1}^{J}\\exp(v_{ij})\\right].$$\n",
    "\n",
    "If we add the monetary amount, $CV$, to all utilities in the baseline,\n",
    "$v_{ij}$, then the expected maximum utility would be the same in the two\n",
    "choice instances, and the agent would thus be indifferent between the\n",
    "two.\n",
    "\n",
    "Policy makers can use the $CV$ measure to compare how much better or\n",
    "worse an individual is from a change in taxation, an introduction or\n",
    "reduction in the choiceset, or a change in one or more of the attributes\n",
    "of the alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The data consists of a survey of households regarding their preferences\n",
    "for car purchase. Each household was given 6 options, but the\n",
    "characteristics that the respondents were asked about was varied. The\n",
    "surveys were originally conducted in order to illicit consumer\n",
    "preferences for alternative-fuel vehicles. The data is *stated\n",
    "preferences*, in the sense that consumers did not actually buy but just\n",
    "stated what they would hypothetically choose, which is of course a\n",
    "drawback. This is very common in marketing when historic data is either\n",
    "not available or does not provide satisfactory variation. The advantage\n",
    "of the stated preference data is therefore that the choice set can be\n",
    "varied greatly (for example, the characteristics includes the\n",
    "availability of recharging stations, which is important for purchase of\n",
    "electric cars).\n",
    "\n",
    "The data you will use has $N=4654$ respondents with $J=6$ cars to choose\n",
    "from. For each household, we let $\\mathcal{J}_{i}$ denote the set of\n",
    "available cars.\n",
    "\n",
    "If you load the mat-file, `car_data.m`, you will get a dictionary with three variables: `y`, `x`, and `labels`. The variable `y` is $N\\times1$ and\n",
    "$\\mathtt{y}_{i}\\in\\{1,2,3,4,5,6\\}$ denotes the chosen car of household\n",
    "$i$. The variable `x` is $N\\times J\\times K$, where $K=21$ is the number\n",
    "of variables (their descriptions given in the table below. The\n",
    "variable `labels` contains the names of the 21 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | **Variable**            | **Description** |\n",
    "|  ---- |------------------------| -------------------------------------------------------------------------------------------------------- |\n",
    "|  1  |  Price/ln(income)       |  Purchase price in thousands of dollars, divided by the natural log of household income in thousands |\n",
    "|  2  |  Range                  |  Hundreds of miles that the vehicle can travel between refuelings/rechargings |\n",
    "|  3  |  Acceleration           |  Seconds required to reach 30 mph from stop, in tens of seconds (e.g., 3 s is entered as 0.3) |\n",
    "|  4  |  Top speed              |  Highest speed that the vehicle can attain, in hundreds of miles/h (e.g., 80 mph is entered as 0.80) |\n",
    "|  5  |  Pollution              |  Tailpipe emissions as fraction of comparable new gas vehicle |\n",
    "|  6  |  Size                   |  0\\\"mini, 0.1\\\"subcompact, 0.2\\\"compact, 0.3\\\"mid-size or large |\n",
    "|  7  |  1(Big enough)          |  1 if household size is over 2 and vehicle size is 3; 0 otherwise |\n",
    "|  8  |  Luggage space          |  Luggage space as fraction of comparable new gas vehicle |\n",
    "  |9  |  Operating cost          | Cost per mile of travel, in tens of cents per mile (e.g., 5 cents/mile is entered as 0.5.)\n",
    " | | |  For electric vehicles, cost is for home recharging. For other vehicles, cost is for station refueling |\n",
    "|  10 |  Station availability   |  Fraction of stations that have capability to refuel/recharge the vehicle |\n",
    "|  11 |  Sports utility vehicle |  1 for sports utility vehicle, zero otherwise |\n",
    "|  12 |  Sports car             |  1 for sports car, zero otherwise |\n",
    "|  13 |  Station wagon          |  1 for station wagon, zero otherwise |\n",
    "|  14 |  Truck                  |  1 for truck, zero otherwise |\n",
    "|  15 |  Van                    |  1 for van, zero otherwise |\n",
    "|  16 |  Constant for EV        |  1 for electric vehicle, zero otherwise |\n",
    "|  17 |  Commute\\*EV            |  1 if respondent commutes less than five miles each day and vehicle is electric; zero otherwise |\n",
    "|  18 |  College\\*EV            |  1 if respondent had some college education and vehicle is electric; zero otherwise |\n",
    "|  19 |  Constant for CNG       |  1 for compressed natural gas vehicle, zero otherwise |\n",
    "|  20 |  Constant for methanol  |  1 for methanol vehicle, zero otherwise |\n",
    "|  21 |  College\\*methanol      |  1 if respondent had some college education and vehicle is methanol; zero otherwise |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-dimensional arrays in Numpy\n",
    "------------------------------\n",
    "\n",
    "The explanatory variables are $x_{ijk}$, where $i$ denotes individuals,\n",
    "$j$ cars, and $k$ car attributes. We have loaded the package `from scipy.io import loadmat`, read its documentation on how we can neatly import `y`, `x` and `labels`. Since Python is 0-indexed, it makes life easier if you take substract 1 from y.\n",
    "\n",
    "Which of the following commands shows all the price-to-log-income\n",
    "    values for the cars in the choiceset of individual 1 (and what does\n",
    "    the other commands give you?).\n",
    "\n",
    "    a:   x[:,1,1]\n",
    "\n",
    "    b:   x[1,:,1]\n",
    "\n",
    "    b:   x[1,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = loadmat('car_data.mat', squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mat_data.get('y')\n",
    "y -= 1\n",
    "x = mat_data.get('x')\n",
    "x_lab = list(mat_data.get('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to index, and make yourself familiar with a three dimensional array."
   ]
  },
  {
   "source": [
    "Next, we want to compute $\\sum_{k=1}^{K}x_{ijk}\\theta_{k}$ using linear algebra. Our  `x` matrix has three dimensions, $N\\times J\\times K$, and $\\theta$ is $K\\times1$. Does this behave as expected when using numpy?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 2\n",
    "\n",
    "Create a 3-dimensional matrix `A` that has dimensions $10 \\times 4 \\times 3$, and fill it with random draws from a normal distribution. What happens if you matrix multiply this by the $3 \\times 1$ vector `1 2 3`? From \"common\" linear algebra knowledge, what do you expect the dimension of the new matrix is?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.default_rng(seed=42)\n",
    "A =  # Create the matrix specified in the question.\n",
    "# Perform some matrix multiplication, does it look like expected?"
   ]
  },
  {
   "source": [
    "Finish the clogit code\n",
    "-----------------------\n",
    "\n",
    "**Question 3.** Finish `simulate_dataset(N,J,theta)` and simulate a dataset. It must return `x` with dimensions ($N\\times J\\times K$) and `y` (or `u`, these are the same, but sometimes I mix the notation) with dimensions ($N\\times1$).\n",
    "\n",
    "To iterate, we are looking at the data generating process in the form of,\n",
    "$$u_{ij} = x_{ij}\\beta + \\varepsilon_{ij},\\quad\\varepsilon_{ij}\\sim\\text{IID Extreme Value Type I}.$$\n",
    "\n",
    "*Programming hint 1:* <br>\n",
    "Now, $x_{ij}\\beta + \\varepsilon_{ij}$ has dimensions ($N\\times J$), which represents the different utilities person $i$ gets from choice $j$. Since person $i$ will choose the option $j$ that have the highest utility, we need for each row get the index of the column that maximises utility (we want the index because we are interested in the choice that maximises utility, but not interested in that utility by itself). In numpy you can use the method `argmax` to get the column number that gives max, look up its documentation on how to use it.\n",
    "\n",
    "*Programming hint 2:* <br>\n",
    "You can draw Extreme Value errors using `genextreme.ppf(q, c)`. Where q is an array of likelihoods (could be some random draws from a uniform distribution), and c specifies which extreme distribution we should draw from. Setting `c=0` gives the extreme value I distribution, which we want to use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "k = 3\n",
    "j = 4\n",
    "thetaTrue = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the nlm.sim_data function.\n",
    "sim_data = nlm.sim_data(n, j, thetaTrue)"
   ]
  },
  {
   "source": [
    "**Question 4.** Finish `choice_prob(theta, x)`. It should return both the `ccp` matrix and the `logccp` matrix.\n",
    "\n",
    "To reiterate, the choice probabilities are given by the function,\n",
    "$$\n",
    "\\Pr(y_{i}=j|\\mathbf{X}_{i})=\\frac{\\exp(x_{ij}\\beta)}{\\sum_{k=1}^{J}\\exp(x_{ik}\\beta)},\n",
    "$$\n",
    "Make sure you reread the section about max rescaling for numerical stability, or else you might have issues later when we do maximum likelihood."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\n"
     ]
    }
   ],
   "source": [
    "# Finish the nlm.choice_prob function, I have made some checks that you can use to make sure you made the function correct.\n",
    "ccp, logccp = nlm.choice_prob(np.zeros(k), sim_data['x'])\n",
    "print((ccp == 0.25).all())\n",
    "print(np.isclose(logccp, -1.38629436).all())"
   ]
  },
  {
   "source": [
    "**Question 5.** Finish the `clogit(theta, y, x)` function. Now that you have written all of these function, you can simulate a fake\n",
    "dataset and check that you can estimate the true parameters back using\n",
    "`estimate()`.\n",
    "\n",
    "*Hint:* Use the function `choice_prob` to get the $N\\times J$ matrix of choice probabilities, `logccp`. You now need to use y vector to get the correct car (in other words, the correct column) that the person choose, and get the choice probability for that car. The nlm.clogit function should return a $N \\times 1$ vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.460613\n         Iterations: 12\n         Function evaluations: 52\n         Gradient evaluations: 13\n"
     ]
    }
   ],
   "source": [
    "sim_results = nlm.estimate(\n",
    "    nlm.clogit, np.zeros((k, )), sim_data['y'], sim_data['x']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results\nDependent variable: y\n\n                   Beta        Se    t-values\n-------------  --------  --------  ----------\ntheta 1 = 1.0  0.961137  0.076385     12.5828\ntheta 2 = 2.0  2.0167    0.108805     18.535\ntheta 3 = 3.0  3.18816   0.158442     20.1219\nIn 12 iterations and 52 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "nlm.print_table(('y', ['theta 1 = 1.0', 'theta 2 = 2.0', 'theta 3 = 3.0']), sim_results)"
   ]
  },
  {
   "source": [
    "Estimation on real data\n",
    "-----------------------\n",
    "**Question 6.** Estimate the 21 parameters of the model. Check that you are able to\n",
    "    find the same estimates and standard errors as in Brownstone and\n",
    "    Train (1998; p. 121).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_res = [-.185 , .350 , -.716 , .261 , -.444 , .935 , .143 , .501 , -.768 , .413 , .820 , .637 , -1.437 , -1.017 , -.799 , -.179 , .198 , .443 , .345 , .313 , .228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, j, k = x.shape\n",
    "theta0 = np.zeros((k, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 1.588275\n         Iterations: 86\n         Function evaluations: 1914\n         Gradient evaluations: 87\nResults\nDependent variable: Car chosen\n\n                          Beta     Se    t-values\n----------------------  ------  -----  ----------\nPrice/ln(income)        -0.185  0.027      -6.817\nRange                    0.350  0.027      12.999\nAcceleration            -0.716  0.111      -6.479\nTop speed                0.261  0.080       3.273\nPollution               -0.444  0.100      -4.436\nSize                     0.934  0.311       3.004\nBig enough               0.143  0.076       1.888\nLuggage space            0.502  0.188       2.668\nOperating cost          -0.768  0.073     -10.466\nStation availability     0.413  0.097       4.278\nSports utility vehicle   0.819  0.144       5.680\nSports car               0.636  0.156       4.068\nStation wagon           -1.437  0.065     -21.962\nTruck                   -1.017  0.055     -18.647\nVan                     -0.799  0.053     -15.023\nConstant for EV         -0.179  0.169      -1.062\nCommute*EV               0.198  0.082       2.409\nCollege*EV               0.443  0.108       4.092\nConstant for CNG         0.345  0.091       3.789\nConstant for methanol    0.313  0.103       3.034\nCollege*methanol         0.229  0.089       2.572\nIn 86 iterations and 1914 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "clogit_results = nlm.estimate(nlm.clogit, theta0, y, x)\n",
    "nlm.print_table(('Car chosen', x_lab), clogit_results, floatfmt='.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# Check that results we get is close to the ones in paper.\n",
    "np.isclose(clogit_results['b_hat'], bt_res, rtol=0.01)"
   ]
  },
  {
   "source": [
    "## Price elasticities\n",
    "As with standard binary probit or logit, the parameter estimates are not\n",
    "interesting in and of themselves. Instead, we want to compute\n",
    "*elasticities* that we can interpret and answer interesting questions\n",
    "with. To do this, recall that an elasticity takes the form\n",
    "\n",
    "$$\\mbox{elasticity}=\\frac{\\mathrm{dy}}{y}\\frac{x}{\\mathrm{d}x}.$$ \n",
    "This\n",
    "formula is useful for computing elasticities numerically: we can\n",
    "increase $x$ by some small amount, say $10^{-5}$, and measure the change\n",
    "in $y$. The elasticity is then the relative change in $y$ divided by the\n",
    "relative change in $x$: $\\frac{\\Delta y/y}{\\Delta x/x}$, or equivalently\n",
    "\n",
    "$$\\text{numerical elasticity}=\\frac{\\text{pct. change in }y}{\\text{pct. change in }x}.$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Question 7.** Compute own-price elasticity of each car, $j=1,...,6$.\n",
    "\n",
    "Here we use the formula\n",
    "\n",
    "$$\n",
    "El_{price}(p_{ij}) = \\frac{price_j}{p_{ij}(x_{ij}, price_j)} \\cdot \\frac{p_{ij}(x_{ij}, price_j \\cdot h) - p_{ij}(x_{ij}, price_j)}{(1 + h)\\cdot price_j}\n",
    "$$\n",
    "\n",
    "where $x_{ij}$ are all the other regressors except for price, and $h = 1 + \\varepsilon$, where $\\varepsilon$ is a very small number, such as $1e-5$\n",
    "\n",
    "Suggestion on how to implement this:\n",
    "- Do these steps by looping over the cars in the data set. So you should create a loop from 0 to 6 (not including 6).\n",
    "- First create a copy of $x$, let us call it $x2$ (make sure that you make a copy, and not a reference). In $x2$ multiply the price of the car (but for all households) by $h$.\n",
    "- Get the choice probabilities using $x2$ and nlm.choice_prob (not the log choice probabilites).\n",
    "- You should now be able to use the formula above to calculate the elasticity, and store it in `E_own`.\n",
    "- When you have looped through all the cars, calculate the mean over all the values to get the average price elasticity.\n",
    "\n",
    "*Note:* `E_own` has dimensions $N \\times J$, so each row is the own price elasticity for some household, and each column hold the own price elasticity for each car."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original choice probabilites\n",
    "ccp, _ = nlm.choice_prob(clogit_results['b_hat'], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_own = np.zeros((n, j))\n",
    "E_cross = np.zeros((n, j))\n",
    "dpdx = np.zeros((n, j))\n",
    "\n",
    "for car in range(j):\n",
    "    # Copy\n",
    "    x2 = x.copy()\n",
    "    x2[:, car, 0] *= 1e-5\n",
    "\n",
    "    ccp2, _ = nlm.choice_prob(clogit_results['b_hat'], x2)\n",
    "    dccp = ccp2 - ccp\n",
    "\n",
    "    x0 = x[:, car, 0]\n",
    "    dx = x2[:, car, 0] - x0\n",
    "\n",
    "    # Own price elasticity\n",
    "    dy = dccp[:, car]\n",
    "    y0 = ccp[:, car]\n",
    "    E_own[:, car] = (x0/dx) * (dy/y0)\n",
    "\n",
    "    # Cross price elasticity\n",
    "    diff_car = [x != car for x in range(j)]\n",
    "    dy = dccp[:, diff_car]\n",
    "    y0 = ccp[:, diff_car]\n",
    "    E_cross[:, car] = np.mean(dy/y0, axis=1) * (x0/dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.6521698318958457\n0.1278177603625842\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(E_own))\n",
    "print(np.mean(E_cross))"
   ]
  },
  {
   "source": [
    "**Question 8.** Compute the cross-price elasticities from each car $j=1,...,6$ on the remaining cars, $k\\ne j$.\n",
    "\n",
    "You perform the same loop as previous question, but instead use only the choice probabilities for all other cars, except for the current car in your loop. You can create a bollean to index the choice probabilites using the function,\n",
    "\n",
    "    diff_car = [x != car for x in range(j)]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Question 9.** Consider the electrical vehicles (EVs) in the dataset. What can you\n",
    "    conclude about the demand for EVs compared to traditional cars? What\n",
    "    does that imply for car producers? And for environmental policy?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.7216596026003235\n-0.6427609949256127\n"
     ]
    }
   ],
   "source": [
    "# Create two indexed, from where idx1 is for electric cars\n",
    "# and idx0 is for non-electric cars.\n",
    "idx1 = x[:, :, 16]==1; \n",
    "idx0 = x[:, :, 16]==0; \n",
    "print(np.mean(E_own[idx1]))\n",
    "print(np.mean(E_own[idx0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}