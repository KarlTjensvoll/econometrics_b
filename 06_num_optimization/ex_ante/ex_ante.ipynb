{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b5806dc0e8395af2278a83b10faec683de35099ebc749f75874e48097d605efb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estimation as est\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "# PS 06 - Numerical Optimization\n",
    "Most estimation procedures involve maximization of some function, e.g.,\n",
    "likelihood functions or (weighted) squared moment conditions. The\n",
    "purpose of this week's exercise is to introduce the *numerical\n",
    "procedures which are used to maximize the objective function* for\n",
    "various $M$-estimators: Maximum Likelihood (ML), Least Absolute\n",
    "Deviations (LAD), etc.\n",
    "\n",
    "Knowing and being able to apply these procedures can help you answer \n",
    "questions like: What can be done if the\n",
    "algorithm does not converge? What can be done in order to speed up the\n",
    "convergence? Did I find the maximum or something else? Is the model\n",
    "identified? Hence, a minimum knowledge about numerical optimization is\n",
    "critical for the applied researcher.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Exercise 1 - Maximizing Univariate Functions\n",
    "==============================================\n",
    "\n",
    "Iterative maximization algorithms\n",
    "---------------------------------\n",
    "\n",
    "The maximization algorithms in this course are all *iterative* in the\n",
    "sense that they start from some initial guess of the parameter,\n",
    "$\\boldsymbol{\\theta}_0$, specified by the researcher. At each iteration, the\n",
    "algorithm moves to a new value of the parameters $\\boldsymbol{\\theta }$ at which\n",
    "the likelihood function, $\\ell \\left( \\boldsymbol{\\theta }\\right)$, is higher\n",
    "than at the previous value of $\\boldsymbol{\\theta }$. Note that the subscripts,\n",
    "thus, are used in this context to express how many iterations the\n",
    "algorithm has run over, i.e., we denote the current value of\n",
    "$\\boldsymbol{\\theta }$ as $\\boldsymbol{\\theta }_{t}$, which is attained after $t$\n",
    "iterations from the starting values $\\boldsymbol{\\theta }_{0}.$ The\n",
    "question is then how to determine the best value of $\\boldsymbol{\\theta} _{t+1}$\n",
    "given $\\boldsymbol{\\theta}_t$?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example: Newton-Raphson (N-R) \n",
    "\n",
    "To determine the best value of $\\boldsymbol{\\theta }_{t+1}$, take a second order\n",
    "Taylor's approximation of $\\ell \\left( \\boldsymbol{\\theta}_{t+1}\\right)$ around $\\ell \\left( \\boldsymbol{\\theta}_{t}\\right) :$\n",
    "\n",
    "$$\\ell \\left( \\boldsymbol{\\theta }_{t+1}\\right) =\\ell \\left( \\boldsymbol{\\theta }_{t}\\right) +\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right)^{\\prime }\\mathbf{g}_{t}+\\frac{1}{2}\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right) ^{\\prime }\\mathbf{H}_{t}\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right)$$ \n",
    "\n",
    "where $\\mathbf{g}_{t}$ and\n",
    "$\\mathbf{H}_{t}$ are the gradient and Hessian of the log--likelihood\n",
    "function evaluated at $\\boldsymbol{\\theta }_{t}$. The gradient equals the\n",
    "average of the scores, i.e., $\\mathbf{g}_{t} \\equiv \\frac{1}{%\n",
    "N}\\sum_{i=1}^{N}\\mathbf{s}_{i}\\left( \\boldsymbol{\\theta }_{t}\\right)$ and $\\mathbf{H}_{t} \\equiv -\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{H}_{i}\\left( \\boldsymbol{\\theta \n",
    "}_{t}\\right)$ (i.e., the subscript $t$ refers to the iteration).\n",
    "\n",
    "Now find the value that maximizes this approximation to $\\ell\n",
    "\\left( \\boldsymbol{\\theta }_{t+1}\\right).$ The first order conditions are\n",
    "\n",
    "$$\n",
    "\\nabla _{\\theta _{t+1}}\\ell \\left( \\boldsymbol{\\theta }_{t+1}\\right) =\\mathbf{g}_{t}+\\mathbf{H}_{t}\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right)\n",
    "\\mathrel{\\mathop :}= \\mathbf{0},\n",
    "$$ \n",
    "\n",
    "(If you struggle with the vector calculus notation; The equation\n",
    "    $\\nabla_{\\theta_{t+1}} \\ell (\\boldsymbol{\\theta}_{t+1}) = \\boldsymbol{0}$ means that\n",
    "    no matter wich of the $K$ parameters in $\\boldsymbol{\\theta}$ you change\n",
    "    marginally, the change in your likelihood should be zero. Also, note\n",
    "    that $\\mathbf{g}_t$ and $\\mathbf{H}_t$ are calculated at\n",
    "    $\\boldsymbol{\\theta}_{\\color{red}{t}}$ so they do not depend on\n",
    "    $\\boldsymbol{\\theta}_{\\color{red}{t+1}}$ (which we are taking FOC wrt.)\n",
    "    and are therefore treated as constants.)\n",
    "\n",
    "with solution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{H}_{t}\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right) &=-\\mathbf{g}_{t} \\\\\n",
    "\\left( \\boldsymbol{\\theta }_{t+1}-\\boldsymbol{\\theta }_{t}\\right) &=-\\mathbf{H}_{t}^{-1}\\mathbf{g}_{t} \\\\\n",
    "\\boldsymbol{\\theta }_{t+1} &=\\boldsymbol{\\theta }_{t}-\\mathbf{H}_{t}^{-1}\\mathbf{g}_{t} \\quad \\quad \\quad (1)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n",
    "This step formalizes the Newton-Raphson (N-R) Algorithm.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Maximizing a quadratic function\n",
    "-------------------------------\n",
    "\n",
    "This exercise asks you to code up the Newton-Raphson algorithm and to\n",
    "apply it to maximize two univariate functions. Consider first the\n",
    "quadratic function $$y\\left( \\theta \\right) =a+b\\theta +c\\theta ^{2}$$\n",
    "\n",
    "for $c>0$ (and note that $\\theta$ is a scalar here). This function takes\n",
    "its maximum at\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}y}{\\mathrm{d}\\theta}=b+2c\\theta =0\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "\\theta _{\\max }=-\\frac{b}{2c}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 1: Calculate first step by hand\n",
    "\n",
    "Consider the quadratic function $y\\left( \\theta\\right)$ for\n",
    "$a=1,b=2,c=3$. Calculate by hand the first step\n",
    "$\\theta_{t+1}-\\theta_{t}$ of the N-R algorithm applied to\n",
    "$y\\left(\\theta\\right)$ starting in $\\theta_{0} = 5.0$. Compare\n",
    "$\\theta_{t+1}$ to the analytical maximum $-\\frac{b}{2c}$. What do\n",
    "you find? Is this surprising? What happens if you change the\n",
    "starting point?\n",
    "\n",
    "*Hint:* To make it clear, from a starting point of 5.0, we find $\\theta_{t + 1}$ by calculating eq. (1). Does it make sense that $g_t$ is the first derivative of $y$ (which is already given to you). And that $H_t$ is the second derivative of $y$?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 2: Solve numerically using code.\n",
    "\n",
    "Now maximize (numerically) the quadratic function\n",
    "$y\\left( \\theta\\right)$ for $a=1,b=2,c=3$ using the N-R algorithm\n",
    "(in Python!). To do it, code up a loop which, starting in\n",
    "$\\theta_{t}$, works out the next step,\n",
    "$\\theta_{t+1}=\\theta_{t}-\\frac{y(\\theta_{t})^\\prime{}}{y(\\theta_{t})^{\\prime{}\\prime{}}}$\n",
    "and ends when some stopping criterion is met. How many iterations\n",
    "are needed to maximize the quadratic function? Try different\n",
    "starting values , $\\theta _{0}$, and see if the required number of iterations changes.\n",
    "Do you get a different result than in Question 1? (Hint: try to\n",
    "compare the forward differenced gradients and Hessian to the\n",
    "analytical one you just computed)\n",
    "\n",
    "*Programming hints:* <br>\n",
    "As shown, we use the function `forward_diff(func, x0, h)` to estimate the gradient and hessian of a function, using forward derivation (the others being backward and centered).\n",
    "\n",
    "Very often when we do minimizing/maximizing, we use `lambda` functions in python. They allow us to make functions \"on the go\". So if we make a lambda function for $y(\\theta)$, and we want to approximately know its gradient at a specific point x, we could do the following,\n",
    "\n",
    "```\n",
    "y = theta: -(a + b*theta + c*theta*theta)\n",
    "grad = estimation.forward_diff(y, x0, h)\n",
    "```\n",
    "\n",
    "You can compute the Hessian by applying\n",
    "`forward_diff(func, x0, h)` twice. Note that since\n",
    "`forward_diff(func, x0, h)` takes a function as input, you\n",
    "need to define an anonymous function that computes the first\n",
    "differensiation, which is done for you. The inner `forward_diff(fun, x0, h)` should\n",
    "use a stepsize equal to the square root of machine precision,\n",
    "$h=1.49e-08$, whereas the outer (first differenc'ing the first\n",
    "difference) should use a much larger one $h=6e-6$ to avoid\n",
    "approximation error.\n",
    "\n",
    "Your loop should end when some convergence criterion is met. To end\n",
    "a loop, we use a `break` statement."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function parameters\n",
    "a = 1.0\n",
    "b = 2.0\n",
    "c = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize machine steps\n",
    "h = np.sqrt(np.finfo(float).eps)  # Square root of machine precision\n",
    "l = 6e-6  # A larger step, or else the hessian might have some issues."
   ]
  },
  {
   "source": [
    "### Question 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "y = lambda theta: a + b*theta + c*theta*theta\n",
    "\n",
    "# First derivate\n",
    "dy = lambda theta: est.forward_diff(y, theta, h)\n",
    "\n",
    "# Second derivate\n",
    "ddy = lambda theta: est.forward_diff(dy, theta, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "x0 = 5.0  # Start value\n",
    "numeval = 10  # Number of iterations\n",
    "tol0 = 1e-8  # Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-4b690057ea82>, line 3)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4b690057ea82>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    grad0 =  # Gradient at point x0\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Fill in the missing code.\n",
    "for i in range(numeval):\n",
    "    grad0 =  # Gradient at point x0\n",
    "    hess0 =  # Hessian at point x0\n",
    "    x_new =  # Calculate eq. (1)\n",
    "    grad_new =  # Gradient in new point\n",
    "    if abs(grad_new)<=tol0:  # If gradient is almost 0, we reached min/max.\n",
    "        break\n",
    "    x0 = x_new  # Replace old point with new point.\n",
    "print(f'Optimal theta reached at {x0:.3f}, with gradient {dy(x0):.2f} and hessian {ddy(x0):.2f}')"
   ]
  },
  {
   "source": [
    "# Question 3 Maximizing the sum of a quadratic and an exponential function\n",
    "\n",
    "Consider now the function\n",
    "$$\n",
    "z\\left( x\\right) =2\\theta^{2}+\\theta^{3}-\\exp \\left( \\theta\\right)\n",
    "$$\n",
    "\n",
    "Question 3.a\n",
    "\n",
    "Maximize $z\\left( \\theta\\right)$ (numerically) for\n",
    "$\\theta\\in \\left[-5,5\\right]$ using the N-R algorithm. Use the starting\n",
    "values $\\theta_{0} \\in \\left\\{ -5,-3,-1,0,1,3,5\\right\\}$. Do you find\n",
    "local minima and maxima? What is the unique global maximum of\n",
    "$z\\left( \\theta\\right)$? Does the Newton-Raphson method fail to converge\n",
    "to the global maximum? Why?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-7ab9126576dc>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-7ab9126576dc>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    z = lambda theta:  # Fill in new equation\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Estimate new functions\n",
    "z = lambda theta:  # Fill in new equation\n",
    "dz = lambda theta:  # Fill in to calculate gradient\n",
    "ddz = lambda theta:  # Fill in to calculate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to start from different start values\n",
    "startvals = [-5.0, -3.0, -1.0, 0.0, 1.0, 3.0, 5.0]\n",
    "n_vals = len(startvals)  # This is to help us to know how many start values to loop over\n",
    "numeval = 100  # Max number of iterations\n",
    "\n",
    "# The following empty lists are to store the values achieved from the different starting points.\n",
    "xopts = np.zeros(n_vals)  # Store optimal x we reached\n",
    "numevals = np.zeros(n_vals)  # Store how many iterations it took\n",
    "fvals = np.zeros(n_vals)  # Store the function value we reached\n",
    "gradients = np.zeros(n_vals)  # Store the gradient value we reached\n",
    "hess = np.zeros(n_vals)  # Store the hessian value we reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in\n",
    "# Make a similar loop like we did for y(theta).\n",
    "# Now, you should have an outer loop, that loops over the different starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Start val       x    f(x)    gradient    hessian    iterations\n-----------  ------  ------  ----------  ---------  ------------\n     -5.000  -1.393   0.930      -0.000     -4.605         7.000\n     -3.000  -1.393   0.930      -0.000     -4.607         7.000\n     -1.000  -1.393   0.930      -0.000     -4.607         8.000\n      0.000   0.273  -1.144       0.000      4.311         8.000\n      1.000   0.273  -1.144       0.000      4.344         6.000\n      3.000  -1.393   0.930      -0.000     -4.604         9.000\n      5.000   4.276  42.799       0.000    -42.324       100.000\n"
     ]
    }
   ],
   "source": [
    "# This prints out a nice table for you.\n",
    "table = []\n",
    "for i in range(n_vals):\n",
    "    row = [startvals[i], xopts[i], fvals[i], gradients[i], hess[i], numevals[i]]\n",
    "    table.append(row)\n",
    "print(tabulate(\n",
    "    table, \n",
    "    headers=['Start val', 'x', 'f(x)', 'gradient', 'hessian', 'iterations'],\n",
    "    floatfmt='.3f'\n",
    "    ))"
   ]
  },
  {
   "source": [
    "You should get something like this:\n",
    "\n",
    "|   Start val |      x |   f(x) |   gradient |   hessian |   iterations |\n",
    "|-------------|--------|--------|------------|-----------|--------------|\n",
    "|      -5.000 | -1.393 |  0.930 |     -0.000 |    -4.605 |        7.000 |\n",
    "|      -3.000 | -1.393 |  0.930 |     -0.000 |    -4.607 |        7.000 |\n",
    "|      -1.000 | -1.393 |  0.930 |     -0.000 |    -4.607 |        8.000 |\n",
    "|       0.000 |  0.273 | -1.144 |      0.000 |     4.311 |        8.000 |\n",
    "|       1.000 |  0.273 | -1.144 |      0.000 |     4.344 |        6.000 |\n",
    "|       3.000 | -1.393 |  0.930 |     -0.000 |    -4.604 |        9.000 |\n",
    "|       5.000 |  4.276 | 42.799 |      0.000 |   -42.324 |      100.000 |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 3.b\n",
    "\n",
    "Make a plot of $z(\\theta)$ around the analytical maximum. What is the\n",
    "problem?\n",
    "\n",
    "*Programming hints:*\n",
    "Use the built-in function `np.linspace` to make a grid around\n",
    "the maximum, evaluate $z(\\theta)$ on this grid and plot it. Even if $\\theta$ is a vector, you should not need to loop over the lambda function $z(\\theta)$ that you made, just pass the linspace vector through it.\n",
    "\n",
    "The function quickly takes some extremely large values. We have therefore made some code that \"zooms\" in on the are that we are interested in.\n",
    "\n",
    "*Note:* when encountering convergence problems, plotting the\n",
    "objective function may be helpful. Therefore, $\\texttt{linspace}$ is\n",
    "a useful function to know."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate on a linespace of x\n",
    "x_axis =  # Fill in using np.linspace\n",
    "y_axis =  # Pass the x_axis through the z() function\n",
    "\n",
    "# Plot the function\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-20, 50)\n",
    "ax.plot(x_axis, y_axis) ;"
   ]
  }
 ]
}