{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "The purpose of this and next week's exercises is to estimate two basic linear panel data models\n",
    "with unobserved effects. The two models make different assumptions about the correlation\n",
    "between observed and unobserved components and it it is important to understand which set\n",
    "of assumptions are the most appropriate in empirical applications. Next week's exercise goes\n",
    "through an econometric test procedure (the Hausman test) that tests the assumptions of the two\n",
    "models against each other. This week's exercise starts out by estimating the unobserved effects\n",
    "model allowing for arbitrary contemporaneous correlation between the unobserved individual\n",
    "effect and the explanatory variables. We shall use two estimators: The Fixed-Effects (FE)\n",
    "estimator and the First-Difference (FD) estimator. <br>\n",
    "\n",
    "Before we start working on some exercises we will briefly introduce two concepts in Python. First, importing and exporting data. Second, using functions. If you are already familiar\n",
    "with these features, you can skip the next two sections and jump directly to the exercises.\n",
    "\n",
    "First, import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from tabulate import tabulate\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and exporting data in Python\n",
    "The easiest way to import data into an numpy array is using a .txt file. Normally we specify a path to the text file, but we will create a fake one to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake file looks like this: \n",
      " 0 1\n",
      " 2 3\n",
      "\n",
      "Loaded into a numpy array, we get the following <class 'numpy.ndarray'>: \n",
      " [[0. 1.]\n",
      " [2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a fake file for easy use.\n",
    "fake_file = StringIO(\"0 1\\n 2 3\")\n",
    "print(f\"Fake file looks like this: \\n {fake_file.getvalue()}\")\n",
    "print()\n",
    "\n",
    "# Load the fake txt file into a numpy array.\n",
    "data = np.loadtxt(fake_file)\n",
    "print(f'Loaded into a numpy array, we get the following {type(data)}: \\n {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, there is no direct way to load an excel sheet into numpy. The easiest solution is to use pandas as an intermediate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# We save the fake file we created earlier as an excel file, so that we can illustrate\n",
    "# how to import using excel.\n",
    "to_export = pd.DataFrame(data)\n",
    "to_export.to_excel('test_file.xlsx', header=None, index=None)\n",
    "\n",
    "# Its important to note that Pandas will treat the first row as a header. If there is no, header\n",
    "# this needs to be specified. There are also alot of extra options to load specific sheets, or\n",
    "# only parts of the sheets and tons of extra options.\n",
    "df_import = pd.read_excel('test_file.xlsx', header=None)\n",
    "np_array = df_import.to_numpy()\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data\n",
    "To save a numpy array as a .txt file is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('real_file.txt', np_array)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If one have large numpy arrays and want to store them efficiently, they can be saved as a binary .npy files. Such files are not compatible with other programs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise with within-groups estimation (FE)\n",
    "\n",
    "Consider the following linear model,\n",
    "\n",
    "$$ y_{it} = \\boldsymbol{x'}_{it}\\boldsymbol{\\beta} + c_i + u_{it}, \\tag{1} $$\n",
    "\n",
    "where $i = 0, ..., N$ indexes the cross sectional unit that is observed (e.g., households), and $t = 0, ..., T$ denotes time (e.g. weeks, years). $\\boldsymbol{x'}_{it}$ is a $K \\times 1$ vector of regressors, $\\boldsymbol{\\beta}$ contains the $K$ parameters to be estimated, $c_i$ is an unobserved individual specifc component which is constant across time periods, and $u_{it}$ is an unobserved random error term.  <br>\n",
    "If $c_i$ turns out to be an additional error term correated with the regressors in the sense of $E[c_i\\boldsymbol{x}_{it}]=0$ for all $t$, then $\\boldsymbol{\\beta}$ can be consistently estimated by pooled OLS (POLS) (as $N\\rightarrow \\infty$ for fixed $T$), albeit in an inefficient manner. To see this, consider the joint error termt $v_{it} = c_i + u_{it}$, and note that,\n",
    "$$E[v_{it}\\boldsymbol{x}_{it}] = E[c_{i}\\boldsymbol{x}_{it}] + E[u_{it}\\boldsymbol{x}_{it}] = \\boldsymbol{0},$$\n",
    "\n",
    "so that the usual OLS assumptions are satisfied. Conversely, if $c$ is systematically related to one or more of the observed variables in the sense of $E[c_{i}\\boldsymbol{x}_{it}] \\neq \\boldsymbol{0}$, then the POLS estimator is _not_ consistent for $\\boldsymbol{\\beta}$.\n",
    "\n",
    "### Example 1. \n",
    "When might $E[c_{i}\\boldsymbol{x}_{it}] \\neq \\boldsymbol{0}$? Consider a model designed to investigate if union membership affects wages. The model explains wages as a function of experience and their union membership.\n",
    "\n",
    "$$ ln(wage_{it}) = \\beta_1 exper_{it} + \\beta_2 exper^2_{it} + \\beta_3 union_{it} + c_i + u_{it}, $$\n",
    "\n",
    "where $c_i$ is an individual-specific constant that summarizes innate and unobserved characteristics. If people select into union or non-union jobs based on which sector rewards their innate characteristics best, then $E[uniont_{it}c_i]\\neq0$. For this reason, it doesn't seem reasonable to use OLS on the pooled data. <br>\n",
    "In this example, the inconsistency of OLS is caused by the presence of $c$. The conventional approach to deal with this problem in linear panel data models is to transform equation (1) such that $c$ vanishes, and the transformed model allows $\\boldsymbol{x}$ to be estimated by OLS. Because the model is linear, we may rid ourselves of $c$ using relatively simple, linear, transformations. In the following, we shall consider two such transformations: i) the _within-groups_ transformation, and ii) the _first-difference_ transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Effects and Within-Groups Transformation\n",
    "\n",
    "The within-groups transformation subtracts from each variable\n",
    "its mean within each cross sectional unit. Consequently, every time-invariant variables disappear when using this transformation. To make the within-groups transformation more explicit, take the average of equation (1) across $T$ for each $i$ to obtain\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y}_{i}=\\mathbf{\\bar{x}}_{i}'\\mathbf{\\beta}+c_{i}+\\bar{u}_{i}, \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{y}_{i}=T^{-1}\\sum_{t=1}^{T}y_{it}$, $\\mathbf{\\bar{x}}_{i}=T^{-1}\\sum_{t=0}^{T}\\mathbf{x}_{it}$,\n",
    "$\\bar{y}_{i}=T^{-1}\\sum_{t=0}^{T}y_{it}$, and $T^{-1}\\sum_{t=0}^{T}c_{i}=c_{i}$.\n",
    "Subtract equation (2) from equation (1) to get\n",
    "\\begin{align}\n",
    "y_{it}-\\bar{y}_{i} & =\\left(\\mathbf{x}_{it}-\\mathbf{\\bar{x}}_{i}\\right)'\\mathbf{\\beta}+(\\color{red}{c_{i}-c_{i}})+\\left(u_{it}-\\bar{u}_{i}\\right) \\\\\n",
    "\\Leftrightarrow\\ddot{y}_{it} & =\\ddot{\\mathbf{x}}_{it}'\\mathbf{\\beta}+\\ddot{u}_{it}. \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "This simple manipulation of the empirical model has eliminated\n",
    "$c_{i}$ by subtracting the mean within each *i*-group. This\n",
    "is called the *within transformation*, and a within-transformed\n",
    "variable is denoted $\\ddot{y}_{it}=y_{it}-\\bar{y}_{i}$. The parameters of interest, $\\boldsymbol{\\beta},$ can be estimated by OLS on the transformed data, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\beta}}_{FE}=(\\mathbf{\\ddot{X}}'\\mathbf{\\ddot{X}})^{-1}\\mathbf{\\ddot{X}}'\\ddot{\\mathbf{y}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{\\ddot{X}}$ is the $NT\\times K$ matrix and $\\ddot{\\mathbf{y}}$ the $NT\\times1$ vector arising from stacking the observables of (3), i.e., $\\ddot{\\mathbf{x}}_{it}'$ and $\\ddot{y}_{it}$, over first $t$ and then $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE Assumptions\n",
    "\n",
    "Let $\\mathbf{\\ddot{X}}_{i}$ denotes the $T\\times K$ matrix arising\n",
    "from stacking $\\ddot{\\mathbf{x}}_{it}'$ over $t$. (We here keep\n",
    "the $i$ subscript to avoid a clash of notation.) We invoke the following assumptions\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "FE.1 & : & E[u_{it}|\\mathbf{x}_{i1},..,\\mathbf{x}_{iT},c_{i}]=0,\\quad\\text{ for }t=1,\\dotsc,T,\\\\\n",
    "FE.2 & : & \\text{Rank }E[\\mathbf{\\ddot{X}}_{i}'\\mathbf{\\ddot{X}}_{i}]=K,\\quad\\text{ for }i=1,\\dotsc,N\\\\\n",
    "FE.3 & : & E[\\mathbf{u}_{i}\\mathbf{u}_{i}'|\\mathbf{x}_{i},c_{i}]=\\sigma_{u}^{2}\\mathbf{I}_{T},\\quad\\text{ for }i=1,\\dotsc,N.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Under the strict exogeneity assumption ($FE.1$) and the rank condition\n",
    "($FE.2$), the FE estimator, $\\hat{\\mathbf{\\beta}}_{FE}$, consistently estimate $\\mathbf{\\beta}$ as $N\\to\\infty$ for fixed $T$. Under FE.3, $\\hat{\\mathbf{\\beta}}_{FE}$ is also asymptotically efficient. (But the latter assumption is not needed for consistency.)\n",
    "\n",
    "In order to perform inference on the obtained parameter\n",
    "estimates, we need standard errors of the parameter estimates. If\n",
    "the unobservables $\\{u_{it}\\}_{t=1}^{T}$ of (1) satisfy\n",
    "$FE.3$, then the variance-covariance matrix of the FE estimator may\n",
    "be estimated by\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{\\mathrm{var}}(\\hat{\\mathbf{\\beta}}_{FE})=\\hat{\\sigma}_{u}^{2}(\\mathbf{\\ddot{X}}'\\mathbf{\\ddot{X}})^{-1},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\sigma}_{u}^{2}:=\\widehat{\\ddot{\\mathbf{u}}}'\\widehat{\\ddot{\\mathbf{u}}}/[N\\left(T-1\\right)-K]$\n",
    "and $\\widehat{\\ddot{\\mathbf{u}}}:=\\ddot{\\mathbf{y}}-\\mathbf{\\ddot{x}}'\\mathbf{\\beta}$ so that $\\widehat{\\ddot{\\mathbf{u}}}'\\widehat{\\ddot{\\mathbf{u}}}=\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\hat{\\ddot{u}}_{it}^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformning data using the `perm` function\n",
    "\n",
    "The main challenge in implementing (3) in Python lies in de-meaning the variables, i.e., constructing $\\ddot{y}_{it}=y_{it}-\\bar{y}_{i}$\n",
    "and $\\mathbf{\\ddot{x}}_{it}=\\mathbf{x}_{it}-\\mathbf{\\bar{x}}_{i}$.\n",
    "On the *individual level*, this can be done by premultiplying equation (1) by a transformation matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Q}_{T}:=\\mathbf{I}_{T}-\\left(\\begin{array}{ccc}\n",
    "1/T & \\ldots & 1/T\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "1/T & \\ldots & 1/T\n",
    "\\end{array}\\right)_{T\\times T}.\n",
    "\\end{equation}\n",
    "\n",
    "However, even though $\\mathbf{Q}_{T}\\mathbf{y}_{i}=\\ddot{\\mathbf{y}}_{i}$, we can't simply multiply the full data vector, $\\mathbf{y}=(\\mathbf{y}_{1},\\dots,\\mathbf{y}_{N})'$, with $\\mathbf{Q}_{T}$ since it needs to be done for each individual. Towards this end, the Python function `perm(P,x)` picks out the elements of the input-vector (here `x`) and premultiplies\n",
    "with the input-matrix `P` for one individual at the time (using\n",
    "a `for` loop). For example, \n",
    "\\begin{align*}\n",
    "`perm`\\left(\\mathbf{Q}_{T},\\begin{pmatrix}\\mathbf{y}_{1}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{y}_{N}\n",
    "\\end{pmatrix}\\right)=\\begin{pmatrix}\\mathbf{Q}_{T}\\mathbf{y}_{1}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{Q}_{T}\\mathbf{y}_{N}\n",
    "\\end{pmatrix}=\\begin{pmatrix}\\ddot{\\mathbf{y}}_{1}\\\\\n",
    "\\vdots\\\\\n",
    "\\ddot{\\mathbf{y}}_{N}\n",
    "\\end{pmatrix} & =\\ddot{\\mathbf{y}}.\n",
    "\\end{align*}\n",
    "\n",
    "The same goes for $\\textbf{x}$-input. (You may want to\n",
    "take a look under the hood of this function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises with FE --- Within-Groups Estimation\n",
    "\n",
    "The exercise takes up the union membership example from before. The data set WAGEPAN.TXT contains information about 545 men who worked every year from 1980 to 1987 in the US. The variables of interest are\n",
    "\n",
    "\n",
    "| Variable | Content |\n",
    "|-|-|\n",
    "| nr | Variable that identifies the individual  |\n",
    "| year | Year of observation |\n",
    "| Black | Black |\n",
    "| Hisp | Hispanic |\n",
    "| Educ | Years of schooling |\n",
    "| Exper | Years since left school |\n",
    "| Expersq | Exper2 |\n",
    "| Married | Marital status |\n",
    "| Union | Union membership |\n",
    "| Lwage | Natural logarithm of hourly wages |\n",
    "\n",
    "Consider the following wage equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln\\left(wage_{it}\\right) & =\\beta_{0}+\\beta_{1}\\textit{exper}_{it}+\\beta_{2}\\textit{exper}_{it}^{2}+\\beta_{3}\\textit{union}_{it}+\\beta_{4}\\textit{married}_{i}\\nonumber \\\\\n",
    " & \\quad+\\beta_{5}\\textit{educ}_{i}+\\beta_{6}\\textit{hisp}_{i}+\\beta_{7}\\textit{black}_{i}+c_{i}+u_{it} \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that *educ, *hisp*, and *black* are time-invariant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE Questions\n",
    "### FE (a):\n",
    "Consider for the moment the unobserved components of (4) as one (composite) error term $v_{it}=c_{i}+u_{it}$ and estimate (4) by pooled OLS. What assumptions are made about $E\\left[c_{i}\\mathbf{x}_{it}\\right]$ and $E\\left[u_{it}\\mathbf{x}_{it}\\right]$ when justifying this estimation approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the data into numpy. \n",
    "data = np.loadtxt('wagepan.txt', delimiter=\",\")\n",
    "id_array = np.array(data[:, 0])\n",
    "\n",
    "# Count how many persons we have. This returns a tuple with the unique IDs,\n",
    "# and the number of times each person is observed.\n",
    "unique_id = np.unique(id_array, return_counts=True)\n",
    "n = unique_id[0].size\n",
    "t = int(unique_id[1].mean())\n",
    "year = np.array(data[:, 1], dtype=int)\n",
    "\n",
    "# Load the rest of the data into arrays.\n",
    "y = np.array(data[:, 8]).reshape(-1, 1)\n",
    "X = np.array(\n",
    "    [np.ones((y.shape[0])), \n",
    "    data[:, 2],\n",
    "    data[:, 4],\n",
    "    data[:, 6],\n",
    "    data[:, 3],\n",
    "    data[:, 9],\n",
    "    data[:, 5],\n",
    "    data[:, 7]]\n",
    ").T\n",
    "\n",
    "# Lets also make some variable names\n",
    "label_y = 'Log wage'\n",
    "label_X = [\n",
    "    'Constant', \n",
    "    'Black', \n",
    "    'Hispanic', \n",
    "    'Education', \n",
    "    'Experience', \n",
    "    'Experience sqr', \n",
    "    'Married', \n",
    "    'Union'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OLS estimator, and then call the function.\n",
    "def estimate_ols(y, X, fe=False, n=0, t=0):\n",
    "    b_hat = la.inv(X.T@X)@(X.T@y)\n",
    "    residual = y - X@b_hat\n",
    "    TSS = (y - np.mean(y)).T@(y - np.mean(y))\n",
    "    SSR = residual.T@residual\n",
    "    ESS = TSS - SSR\n",
    "    R2 = np.array(ESS/TSS)\n",
    "\n",
    "    # If we are estiamting a FE model, we need to correct sigma.\n",
    "    if not fe:\n",
    "        sigma = np.array(SSR/(X.shape[0] - X.shape[1]))\n",
    "    else:\n",
    "        sigma = np.array(SSR/(n*(t - 1) - X.shape[1]))\n",
    "    b_var = sigma*la.inv(X.T@X)\n",
    "    b_std = np.sqrt(b_var.diagonal()).reshape(-1, 1)\n",
    "    t_values = b_hat/b_std\n",
    "\n",
    "    return b_hat, b_std, sigma, t_values, R2\n",
    "b_hat, b_std, sigma, t_values, R2 = estimate_ols(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled OLS\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                  Beta hat     Std    T value\n",
      "--------------  ----------  ------  ---------\n",
      "Constant           -0.0347  0.0646    -0.5375\n",
      "Black              -0.1438  0.0236    -6.1055\n",
      "Hispanic            0.0157  0.0208     0.7543\n",
      "Education           0.0994  0.0047    21.2476\n",
      "Experience          0.0892  0.0101     8.8200\n",
      "Experience sqr     -0.0028  0.0007    -4.0272\n",
      "Married             0.1077  0.0157     6.8592\n",
      "Union               0.1801  0.0171    10.5179\n",
      "R² = 0.187\n",
      "σ² = 0.231\n"
     ]
    }
   ],
   "source": [
    "# Print the table\n",
    "def print_table(headers, title, label_X, label_y, b_hat, b_std, t_values):\n",
    "    table = []\n",
    "    for i, name in enumerate(label_X):\n",
    "        table_row = [name, b_hat[i], b_std[i], t_values[i]]\n",
    "        table.append(table_row)\n",
    "        \n",
    "    print(title)\n",
    "    print(f'Dependent variable: {label_y}\\n')\n",
    "    print(tabulate(table, headers, floatfmt='.4f'))\n",
    "    print(f'R\\u00b2 = {R2[0, 0]:.3f}')\n",
    "    print(f'\\u03C3\\u00b2 = {sigma[0, 0]:.3f}' )\n",
    "\n",
    "headers = ['', 'Beta hat', 'Std', 'T value']\n",
    "title = 'Pooled OLS'\n",
    "print_table(headers, title, label_X, label_y, b_hat, b_std, t_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE (b):\n",
    "Within transform the data. What happens to *educ, hisp, and black* and $x_{it1}\\equiv1$ when the data are within transformed? What is the rank of the within transformed $\\mathbf{X}$ matrix? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demeaning_matrix(t):\n",
    "    Q_T = np.eye(t) - np.tile(1/t, (t, t))\n",
    "    return Q_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm(Q_T, Z):\n",
    "    '''\n",
    "        Q_T is the transformation matrix.\n",
    "        Z is the matrix that is to be transformed.\n",
    "    '''\n",
    "    # We can infer t from the shape of the transformation matrix.\n",
    "    t = Q_T.shape[0]\n",
    "\n",
    "    # Initialize the numpy array\n",
    "    A = np.zeros(Z.shape)\n",
    "\n",
    "    # Loop over the individuals, and permutate their values.\n",
    "    for i in range(int(Z.shape[0]/t)):\n",
    "        A[i*t : (i + 1)*t] = Q_T@Z[i*t : (i + 1)*t]\n",
    "    return A\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of demeaned X: 4\n",
      "Eigenvalues of within-transformed X: [4248875.    1871.     365.     329.       0.       0.       0.       0.]\n"
     ]
    }
   ],
   "source": [
    "Q_T = demeaning_matrix(t)\n",
    "y_demean = perm(Q_T, y)\n",
    "X_demean = perm(Q_T, X)\n",
    "\n",
    "# Check rank of demeaned matrix, and return its eigenvalues.\n",
    "def check_rank(X):\n",
    "    print(f'Rank of demeaned X: {la.matrix_rank(X)}')\n",
    "    lambdas, V = la.eig(X.T@X)\n",
    "    np.set_printoptions(suppress=True)  # This is just to print nicely.\n",
    "    print(f'Eigenvalues of within-transformed X: {lambdas.round(decimals=0)}')\n",
    "check_rank(X_demean)\n",
    "\n",
    "# We need to drop the columns that are constant over time.\n",
    "# Try to first estimate the OLS without removing the 0-columns.\n",
    "X_demean_non_singular = X_demean[:, 4:]\n",
    "label_X_non_singular = label_X[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE (c):\n",
    "Estimate (4) on within transformed data (make sure that the employed $\\mathbf{\\ddot{X}}$ has full rank - drop columns if necessary). How big is the union premium according to the estimate from the FE model? Compare this with the estimate that you calculated from the pooled OLS regression. What does this suggest about $E\\left[union_{it}c_{i}\\right]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                  Beta hat     Std    T value\n",
      "--------------  ----------  ------  ---------\n",
      "Experience          0.1168  0.0084    13.8778\n",
      "Experience sqr     -0.0043  0.0006    -7.1057\n",
      "Married             0.0453  0.0183     2.4743\n",
      "Union               0.0821  0.0193     4.2553\n",
      "R² = 0.178\n",
      "σ² = 0.123\n"
     ]
    }
   ],
   "source": [
    "# Estimate FE OLS using the demeaned variables.\n",
    "b_hat, b_std, sigma, t_values, R2 = estimate_ols(\n",
    "    y_demean, X_demean_non_singular, fe=True, n=n, t=t\n",
    ")\n",
    "title = 'FE'\n",
    "print_table(headers, title, label_X_non_singular, label_y, b_hat, b_std, t_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises with first-difference estimation (FD)\n",
    "\n",
    "The within transformation is one particular transformation\n",
    "that enables us to get rid of $c_{i}$. An alternative is the first-difference transformation. To see how it works, lag Equation (1) one period and subtract it from (1) such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta y_{it}=\\Delta\\mathbf{x}_{it}'\\mathbf{\\beta}+\\Delta u_{it},\\quad t=\\color{red}{2},\\dotsc,T, \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Delta y_{it}:=y_{it}-y_{it-1}$, $\\Delta\\mathbf{x}_{it}:=\\mathbf{x}_{it}-\\mathbf{x}_{it-1}$\n",
    "and $\\Delta u_{it}:=u_{it}-u_{it-1}$. As was the case for the within\n",
    "transformation, first differencing eliminates the time invariant component\n",
    "$c_{i}$. Note, however, that one time period is lost when differencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD Assumptions\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "FD.1 & : & E[u_{it}|\\mathbf{x}_{i1},..,\\mathbf{x}_{iT},c_{i}]=0 \\; \\; \\; \\; t=1,\\dots,T, \\; \\;  \\text{(as in }FE.1\\text{)} \\\\\n",
    "FD.2 & : & \\text{Rank }E[\\Delta\\mathbf{x}_{i}\\Delta\\mathbf{x}_{i}']=K,\\quad\\text{ for }i=1,\\dots,N,\\\\\n",
    "FD.3 & : & E[\\mathbf{e}_{i}\\mathbf{e}_{i}'|\\mathbf{x}_{i},c_{i}]=\\sigma_{u}^{2}\\mathbf{I}_{T-1} \\; \\text{ with }\\mathbf{e}_{i}:=\\Delta\\mathbf{u}_{i},\\quad\\text{ for }i=1,\\dots,N.\n",
    "\\end{eqnarray*}\n",
    "Under the strict exogeneity assumption ($FD.1$) and the rank condition ($FD.2$), the FD estimator\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_{FD}=\\left(\\Delta\\mathbf{X}\\Delta\\mathbf{X}\\right)^{-1}\\Delta\\mathbf{X}^{\\prime}\\Delta\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "consistently estimates $\\boldsymbol{\\beta}$ (as $N\\to\\infty$ for\n",
    "fixed $T$). If also FD.3 holds, then $\\hat{\\boldsymbol{\\beta}}_{FD}$\n",
    "is asymptotically efficient. (Again, the latter assumption is not\n",
    "needed for consistency.)\n",
    "\n",
    "Under $FD.3$, $u_{it}=u_{it-1}+e_{it}$ follows a random walk. This\n",
    "is the opposite extreme relative to assumption $FE.3$, where the\n",
    "$u_{it}$ are assumed to be serially uncorrelated. In many cases,\n",
    "the truth is likely to lie somewhere in between. The variance-covariance\n",
    "matrix of the FE estimator may be estimated by\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{\\text{var}}(\\hat{\\mathbf{\\beta}}_{FD})=\\hat{\\sigma}_{e}^{2}\\left(\\Delta\\mathbf{X}'\\Delta\\mathbf{X}\\right)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\sigma}_{e}^{2}:=\\hat{\\mathbf{e}}^{\\prime}\\hat{\\mathbf{e}}/[N\\left(T-1\\right)-K]$\n",
    "and $\\hat{e}_{it}:=$ $\\Delta y_{it}-\\Delta\\mathbf{x}_{it}'\\widehat{\\mathbf{\\beta}}$.\n",
    "\n",
    "Notice how we, both in the case of FE and FD, manipulate the model\n",
    "in a way that allows the standard OLS assumptions to hold on the *transformed* data, and then simply treat the transformed model as if it was our model of interest. Under exogeneity ($FE.1/FD.1$) the choice between first difference and the within estimator pertains to efficiency considerations, and the choice hinges on the assumptions made about the serial correlation of the errors $(FE.3$$/$$FD.3)$.\n",
    "\n",
    "To estimate the coefficients in (5) in Python, we must first\n",
    "difference all the variables, i.e construct $\\Delta y_{it}=y_{it}-y_{it-1}$ and $\\Delta\\mathbf{x}_{it}=\\mathbf{x}_{it}-\\mathbf{x}_{it-1}$. This can be done by premultiplying the variables in levels ($y_{i}$ and $\\mathbf{x}_{i})$ by the transformation matrix $\\mathbf{D}$ given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{D}:=\\left(\\begin{array}{cccccc}\n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "-1 & 1 & 0 & \\ldots & 0 & 0\\\\\n",
    "0 & -1 & 1 &  & 0 & 0\\\\\n",
    "\\vdots &  &  & \\ddots &  & \\vdots\\\\\n",
    "0 & 0 & 0 & \\ldots & -1 & 1\n",
    "\\end{array}\\right)_{T\\times T}.\n",
    "\\end{equation}\n",
    "\n",
    "(Can you see why $\\mathbf{D}$ gets the job done?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD Questions\n",
    "### FD (a):\n",
    "Construct $\\mathbf{D}$ and use the procedure `perm` $(\\mathbf{D},\\mathbf{x})$ to compute first differences of the elements of $\\mathbf{y}$ and $\\mathbf{x}$. What happens to *educ, hisp* and *black* and $x_{it1}\\equiv1$ when the data are transformed into first differences? What is the rank of the first differenced $\\mathbf{x}$-matrix? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_matrix(n):\n",
    "    D_T = np.eye(8) - np.eye(8, k=-1)\n",
    "    D_T[0, :] = 0\n",
    "    return D_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of demeaned X: 4\n",
      "Eigenvalues of within-transformed X: [753711.    356.    545.    508.      0.      0.      0.      0.]\n"
     ]
    }
   ],
   "source": [
    "# Transform the data.\n",
    "D_T = fd_matrix(n)\n",
    "y_diff = perm(D_T, y)\n",
    "X_diff = perm(D_T, X)\n",
    "\n",
    "# Again, check rank condition.\n",
    "check_rank(X_diff)\n",
    "\n",
    "# Remember to remove the first observation for each person (which is year 1980).\n",
    "# Not strictly necessary?\n",
    "y_diff = y_diff[year != 1980]\n",
    "X_diff = X_diff[year != 1980]\n",
    "\n",
    "# Remember to remove linear dependent columns\n",
    "X_diff_non_singular = X_diff[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD (b):\n",
    "Estimate (4) in first differences. How big is the union premium according to the estimate from this model? Compare the FD estimate with the estimate that you calculated from the FE regression. Is there a difference? If yes, what (if anything) can we conclude based on this finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                  Beta hat     Std    T value\n",
      "--------------  ----------  ------  ---------\n",
      "Experience          0.1158  0.0196     5.9096\n",
      "Experience sqr     -0.0039  0.0014    -2.8005\n",
      "Married             0.0381  0.0229     1.6633\n",
      "Union               0.0428  0.0197     2.1767\n",
      "R² = 0.004\n",
      "σ² = 0.196\n"
     ]
    }
   ],
   "source": [
    "b_hat, b_std, sigma, t_values, R2 = estimate_ols(y_diff, X_diff_non_singular)\n",
    "title = 'FD'\n",
    "print_table(headers, title, label_X_non_singular, label_y, b_hat, b_std, t_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise comparing FE and FD\n",
    "### Question FE v. FD (a):\n",
    "Test for serial correlation in the errors using an auxilliary AR(1) model, to test assumption FD.3, where the errors $e_{it} = \\Delta u_{it}$ should be serially uncorrelated.\n",
    "\n",
    "We can easily test this assumption given the OLS residuals from equation (5). Run the regression (note that you will loose data for\n",
    "the *two* first periods)\n",
    "\\begin{equation}\n",
    "\\hat{e}_{it}=\\rho\\hat{e}_{it-1}+error_{it},\\quad t=\\color{red}{3},\\dotsc,T,\\quad i=1,\\dotsc,N\n",
    "\\end{equation}\n",
    "\n",
    "Do you find any evidence for serial correlation? Does FD.3 seem appropriate? And why don't we include an intercept in this auxilliary equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Under FE.3, the idiosyncratic errors $u_{it}$\n",
    "are uncorrelated. However, FE.3 implies that the $e_{it}$'s are autocorrelated. In fact, of the $u_{it}$'s are serially uncorrelated to beging with, corr$\\left(e_{it},e_{it-1}\\right)=-0.5$. (Check!) This test is of course only valid if the explanatory variables are strictly exogenous!\n",
    "\n",
    "*Hint:* You can use the `perm` function to lag\n",
    "the error term variable. Consider the following; \n",
    "\\begin{align*}\n",
    "\\underset{T\\times T}{\\begin{pmatrix}0 & 0 & 0 & \\cdots & 0 & 0\\\\\n",
    "1 & 0 & 0 & \\cdots & 0 & 0\\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\cdots & 1 & 0\n",
    "\\end{pmatrix}}\\underset{1\\times T}{\\begin{pmatrix}y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{T}\n",
    "\\end{pmatrix}}=\\underset{1\\times T}{\\begin{pmatrix}y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{pmatrix}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_correlation(y, X, year):\n",
    "    # We often use _ for a variable that we are not interested in,\n",
    "    # but is returned anywa.\n",
    "    b_hat, _, _, _, _ = estimate_ols(y, X)\n",
    "    e = y - X@b_hat\n",
    "    \n",
    "    # Create a lag to estimate the error on.\n",
    "    L_T = np.eye(y.size, k=-1)\n",
    "    e_x = perm(L_T, e)\n",
    "\n",
    "    # We then need to remove the first obs for every person again.\n",
    "    reduced_year = year[year != 1980]\n",
    "    e = e[reduced_year != 1981]\n",
    "    e_l = e_x[reduced_year != 1981]\n",
    "\n",
    "    return estimate_ols(e, e_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Correlation\n",
      "Dependent variable: OLS residual, eᵢₜ\n",
      "\n",
      "         Beta hat     Std    T value\n",
      "-----  ----------  ------  ---------\n",
      "eᵢₜ₋₁     -0.3961  0.0147     7.0843\n",
      "R² = 0.182\n",
      "σ² = 0.143\n"
     ]
    }
   ],
   "source": [
    "b_hat, b_std, sigma, t_values, R2 = serial_correlation(y_diff, X_diff_non_singular, year)\n",
    "# Replace the t-value.\n",
    "\n",
    "t_values[0] = (b_hat[0] + 0.5)/b_std[0]\n",
    "\n",
    "label_y = 'OLS residual, e\\u1d62\\u209c'\n",
    "label_e = ['e\\u1d62\\u209c\\u208B\\u2081']\n",
    "title = 'Serial Correlation'\n",
    "print_table(headers, title, label_e, label_y, b_hat, b_std, t_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question FE v FD (b):\n",
    "Add interactions on the form $d_{81}\\cdot educ, d_{82}\\cdot educ, ..., d_{87}\\cdot educ$ and estimate the model with fixed effect. Has the return to education increased over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint:* Remember that $educ_{i}$ doesn't vary over\n",
    "time! Therefore we didn't use $educ$ in levels in the FE estimation.\n",
    "However, if we suppose that the structural equation (4) contains a term $\\sum_{s=2}^{T}\\delta_{s}d_{s}educ_{i}$, it will be perfectly fine to within-transform these interactions since they vary over time (although in a highly structured manner - they equal\n",
    "zero in all time periods but one, and then $educ$). Note that one\n",
    "period is dropped for the within-transformation to work whereas the\n",
    "levels term, $\\beta_{5}educ_{i}$, is dropped to avoid producing a\n",
    "constant row.\n",
    "\n",
    "*Programming hint:* You want to append the dataset with a dummy matrix, that would look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "14 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 14 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 14 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 9 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This example shows our two first persons, that have 14 and 9 years of education respectively. This matrix can be created as a product of two matrices, what would they look like? Why is the first row only zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This dummy block has a 0 row, as we need to exclude one\n",
    "# year in order to not end up in the dummy trap.\n",
    "dummy_block = np.eye(t, k=-1)[:, :-1]\n",
    "\n",
    "# Expand thid dummy block to all persons\n",
    "dummy_matrix = np.tile(dummy_block, (n, 1))\n",
    "\n",
    "# We now create a n*t-1 matrix, it with the person's education \n",
    "expanded_educ = np.transpose([X[:, 3]] * (t-1))\n",
    "\n",
    "# We can now multiplu the year dummy with a person's education\n",
    "educ_dummies = dummy_matrix*expanded_educ\n",
    "educ_demean = perm(Q_T, educ_dummies)\n",
    "X_demean_dummies = np.hstack([X_demean_non_singular, educ_demean])\n",
    "\n",
    "label_x_interactions = label_X_non_singular + ['E81', 'E82', 'E83', 'E84', 'E85', 'E86', 'E87']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE with year interactions\n",
      "Dependent variable: OLS residual, eᵢₜ\n",
      "\n",
      "                  Beta hat     Std    T value\n",
      "--------------  ----------  ------  ---------\n",
      "Experience          0.1705  0.0273     6.2462\n",
      "Experience sqr     -0.0060  0.0009    -6.9581\n",
      "Married             0.0475  0.0183     2.5925\n",
      "Union               0.0794  0.0193     4.1138\n",
      "E81                -0.0010  0.0026    -0.4009\n",
      "E82                -0.0062  0.0041    -1.5224\n",
      "E83                -0.0114  0.0057    -2.0006\n",
      "E84                -0.0136  0.0072    -1.8787\n",
      "E85                -0.0162  0.0087    -1.8578\n",
      "E86                -0.0170  0.0101    -1.6804\n",
      "E87                -0.0167  0.0115    -1.4619\n",
      "R² = 0.181\n",
      "σ² = 0.123\n"
     ]
    }
   ],
   "source": [
    "b_hat, b_std, sigma, t_values, R2 = estimate_ols(y_demean, X_demean_dummies, fe=True, n=n, t=t)\n",
    "title = 'FE with year interactions'\n",
    "print_table(headers, title, label_x_interactions, label_y, b_hat, b_std, t_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
