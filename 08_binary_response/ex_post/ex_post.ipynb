{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the LinearModels module\n",
    "# But first we need to make sure that we look for modules one folder up.\n",
    "from sys import path\n",
    "path.append('../')\n",
    "import NonLinearModels_post as nlm\n",
    "import LinearModels as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy.stats import norm\n",
    "from scipy import optimize\n",
    "from tabulate import tabulate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 08: Non-linear models\n",
    "## Labor participation of married women\n",
    "\n",
    "The goal of this week's problem set is to investigate the labor participation of \n",
    "married women, using three different types of binary response models.\n",
    "Binary response models are relevant when the dependent variable $y$ has two possible outcomes, \n",
    "e.g., $y=1$ if a person participates in the labor force, and $y=0$ if she does not.\n",
    "The three models that you are asked to estimate are the Linear Probability Model (LPM), \n",
    "the Probit model and the Logit model. \n",
    "\n",
    "_Note:_ This week, most of the code has been created for you - you just need to fill in some blanks in the module `NonLinearModel.py`. To estimate the LPM-model using OLS, we will use the code that we have already used in the course, which is in the `LinearModels.py` file, and is preloaded as `lm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To conduct your analysis, you will use data coming from the following article, and reproduce\n",
    "some of its results: \n",
    "\n",
    "> Michael Gerfin (1996): \"Parametric and Semi-Parametric Estimation of the Binary Response Model of Labour Market Participation\", _Journal of Applied Econometrics_ , Vol. 11, Issue 3, pp. 321-339, [DOI link](https://doi.org/10.1002/(SICI)1099-1255(199605)11:3%3C321::AID-JAE391%3E3.0.CO;2-K)\n",
    "\n",
    "This article compares parametric and semiparametric methods for the estimation of binary choice\n",
    "models, using two different data sets for Swiss and German women. In this assignment, you will\n",
    "only work with the Swiss data, and implement parametric methods - we will discuss semiparametric \n",
    "methods in a later lecture. \n",
    "\n",
    "The data set $\\texttt{swiss.txt}$ contains information about 872 women, of which 401\n",
    "participate in the labor market (The data set was obtained from the Journal of Applied Econometrics Data Archive\n",
    "at http://qed.econ.queensu.ca/jae/1996-v11.3/gerfin/.\n",
    "\n",
    "\n",
    "The variables are defined in the table below (see, also, section 3 page 326 of the article).\n",
    "\n",
    "|Var | Definition |\n",
    "|--|--|\n",
    "| $\\texttt{LFP}     $|  = 1 if in labor force, 0 otherwise | \n",
    "| $\\texttt{AGE}     $|  age in years (divided by 10) | \n",
    "| $\\texttt{EDUC}    $|  number of years of formal education | \n",
    "| $\\texttt{NYC}     $|  number of young children | \n",
    "| $\\texttt{NOC}     $|  number of older children | \n",
    "| $\\texttt{NLINC}   $|  logarithm of yearly non-labor income | \n",
    "| $\\texttt{FOREIGN} $|  = 1 if permanent foreign resident, 0 otherwise |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells load the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data =  np.loadtxt('swiss.txt')\n",
    "n = data.shape[0]\n",
    "\n",
    "lfp = data[:, 0].reshape(-1, 1)\n",
    "nlinc = data[:, 1].reshape(-1, 1)\n",
    "age = data[:, 2].reshape(-1, 1)\n",
    "agesq = np.power(age, 2).reshape(-1, 1)\n",
    "educ = data[:, 3].reshape(-1, 1)\n",
    "nyc = data[:, 4].reshape(-1, 1)\n",
    "noc = data[:, 5].reshape(-1, 1)\n",
    "foreign = data[:, 6].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "y = lfp \n",
    "ones = np.ones((n, 1))\n",
    "\n",
    "x = np.hstack((ones, age, agesq, educ, nyc, noc, nlinc, foreign))\n",
    "k = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare labels\n",
    "y_lab = 'lfp'\n",
    "x_lab = [\n",
    "    'const', 'age', 'agesq', 'educ', 'nyc', 'noc', 'nlinc', 'foreign'\n",
    "]"
   ]
  },
  {
   "source": [
    "### Question 1: Estimate model using LPM\n",
    "We model Labour participation of females using an LPM model, which we estimate using OLS. Use the given `lm` module, and print it out in a nice table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LPM results\nDependent variable: lfp\n\n            Beta      Se    t-values\n-------  -------  ------  ----------\nconst     1.6637  0.3973      4.1880\nage       0.6825  0.1200      5.6891\nagesq    -0.0970  0.0145     -6.6821\neduc      0.0067  0.0058      1.1502\nnyc      -0.2406  0.0301     -8.0031\nnoc      -0.0493  0.0174     -2.8291\nnlinc    -0.2128  0.0355     -5.9892\nforeign   0.2496  0.0402      6.2134\nR² = 0.193\nσ² = 0.202\n"
     ]
    }
   ],
   "source": [
    "ols_results = lm.estimate(y, x)\n",
    "lm.print_table(\n",
    "    (y_lab, x_lab), ols_results, \n",
    "    title='LPM results', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Estimate the binary outcome using the probit model, but first, some theoretical foundation.\n",
    "\n",
    "For binary outcome data the dependent variable takes on the values,\n",
    "\n",
    "$$\n",
    "y=\\left\\{ \\begin{array}{c}\n",
    "\\hspace{-1.75em}1\\quad\\text{with prob. }p,\\\\\n",
    "0\\quad\\text{with prob. }1-p.\n",
    "\\end{array}\\right.\n",
    "$$ \n",
    "\n",
    "We can then specify a regression model by\n",
    "parameterizing the probability $p$ to depend on a regressor vector, $x$,\n",
    "which is $N\\times K$ and a parameter vector, $\\beta$, which is\n",
    "$K\\times 1$. The conditional probability is given as,\n",
    "$$\n",
    "p_{i}=P\\left(y_{i}=1\\left|x\\right.\\right)=G\\left(x_{i}^{\\prime}\\beta\\right)\n",
    "$$\n"
   ]
  },
  {
   "source": [
    "### The Probit Model\n",
    "\n",
    "For the Probit model the response probability is non-linear,\n",
    "$$\n",
    "P\\left(y_{i}=1\\left|x\\right.\\right)=G\\left(x^{\\prime}\\beta\\right)=\\Phi\\left(x^{\\prime}\\beta\\right)=\\int_{-\\infty}^{x^{\\prime}\\beta}\\phi\\left(z\\right)dz, \\tag{1}\n",
    "$$\n",
    "where $\\Phi\\left(\\cdot\\right)$ is the standard normal cdf, with\n",
    "derivative,\n",
    "$$\n",
    "\\phi\\left(z\\right)=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\exp\\left(\\frac{-z^{2}}{2}\\right)\n",
    "$$\n",
    "which is the standard normal density function.\n",
    "\n",
    "The outcome in binary models is Bernoulli distributed (the binomial\n",
    "distribution with only one trial). The probability mass function for\n",
    "$y_{i}$ is\n",
    "$$\n",
    "y_{i}=f\\left(y_{i}\\left|x_{i}\\right.\\right)=p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\\quad y_{i} \\in \\{0,1\\}.\n",
    "$$\n",
    "The log-likelihood contribution for observation $i$ is,\n",
    "$$\n",
    "L\\left(\\beta\\right)=y_{i}\\log G\\left(x_{i}^{\\prime}\\beta\\right)+\\left(1-y_{i}\\right)\\log\\left(1-G\\left(x_{i}^{\\prime}\\beta\\right)\\right)\n",
    "$$\n",
    "And the log--likelihood function is, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_{N}\\left(\\beta\\right)=\\sum_{i=1}^{N}\\left\\{ y_{i}\\log G\\left(x_{i}^{\\prime}\\beta\\right)+\\left(1-y_{i}\\right)\\log\\left(1-G\\left(x_{i}^{\\prime}\\beta\\right)\\right)\\right\\}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the empty parts in the probit_citerion() function. This takes some best guess beta parameters (in this case the beta parameters from a LPM is a good first guess), and the data values y and x. It returns a vector of likelihood \n",
    "\n",
    "# z should be the input of G, what does eq. (1) suggest that z is?\n",
    "# What does eq. (1) suggets that the functional form of G?\n",
    "# For G you can use scipy's norm.cdf()\n",
    "# Finaly, write up the function for the likelihood contribution and return it\n",
    "\n",
    "# You can check with the cell below that you have written the probit_criterion correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "np.isclose(np.sum(nlm.probit_criterion(ols_results['b_hat'], y, x)), -633.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.583231\n         Iterations: 36\n         Function evaluations: 360\n         Gradient evaluations: 40\n"
     ]
    }
   ],
   "source": [
    "probit_result = nlm.estimate(\n",
    "    nlm.probit_criterion, ols_results['b_hat'], y, x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LPM results\nDependent variable: lfp\n\n            Beta      Se    t-values\n-------  -------  ------  ----------\nconst     1.6637  0.3973      4.1880\nage       0.6825  0.1200      5.6891\nagesq    -0.0970  0.0145     -6.6821\neduc      0.0067  0.0058      1.1502\nnyc      -0.2406  0.0301     -8.0031\nnoc      -0.0493  0.0174     -2.8291\nnlinc    -0.2128  0.0355     -5.9892\nforeign   0.2496  0.0402      6.2134\nR² = 0.193\nσ² = 0.202\n"
     ]
    }
   ],
   "source": [
    "lm.print_table(\n",
    "    (y_lab, x_lab), ols_results, \n",
    "    title='LPM results', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.45078840709740686"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "norm.cdf(np.mean(x, axis=0) @ probit_result['b_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probit results\nDependent variable: lfp\n\n           Beta     Se    t-values\n-------  ------  -----  ----------\nconst     3.749  1.495       2.508\nage       2.075  0.417       4.978\nagesq    -0.294  0.051      -5.783\neduc      0.019  0.018       1.062\nnyc      -0.714  0.096      -7.417\nnoc      -0.147  0.050      -2.922\nnlinc    -0.667  0.137      -4.861\nforeign   0.714  0.121       5.920\nIn 36 iterations and 360 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "nlm.print_table(\n",
    "    (y_lab, x_lab), probit_result, \n",
    "    title='Probit results', floatfmt='.3f'\n",
    ")"
   ]
  },
  {
   "source": [
    "Your table should look aprox. this:\n",
    "\n",
    "Probit results <br>\n",
    "Dependent variable: lfp\n",
    "\n",
    "|         |   Beta |    Se |   t-values |\n",
    "|---------|--------|-------|------------|\n",
    "| const   |  3.749 | 1.495 |      2.508 |\n",
    "| age     |  2.075 | 0.417 |      4.978 |\n",
    "| agesq   | -0.294 | 0.051 |     -5.783 |\n",
    "| educ    |  0.019 | 0.018 |      1.062 |\n",
    "| nyc     | -0.714 | 0.096 |     -7.417 |\n",
    "| noc     | -0.147 | 0.050 |     -2.922 |\n",
    "| nlinc   | -0.667 | 0.137 |     -4.861 |\n",
    "| foreign |  0.714 | 0.121 |      5.920 |\n",
    "In 36 iterations and 360 function evaluations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Compare your results to those published in Gerfin (1996, p. 327 Table I) for the Probit model, see the table below. Interpret and compare the results from the two estimation approaches, both from a statistical and an economic point of view.\n",
    "\n",
    "| Variable | $\\hat{\\beta}$ | s.e |\n",
    "|----|---|---|\n",
    "| $\\texttt{CONST}   $|  3.75\t| (1.41)  |\n",
    "| $\\texttt{AGE}     $|  2.08\t| (0.41)  |\n",
    "| $\\texttt{AGESQ}   $|  -0.29|& (0.05) | \n",
    "| $\\texttt{EDUC}    $|  0.02\t| (0.02)  |\n",
    "| $\\texttt{NYC}     $|  -0.71|& (0.10) | \n",
    "| $\\texttt{NOC}     $|  -0.15|& (0.05) | \n",
    "| $\\texttt{NLINC}   $|  -0.67|& (0.13) | \n",
    "| $\\texttt{FOREIGN} $|   0.71| & (0.12)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Estimate the logit model with maximum likelihood, using the same explanatory variables as in\n",
    "**Question 3**.\n",
    "\n",
    "### The Logit Model\n",
    "\n",
    "For the Logit model the response probability is non-linear,\n",
    "$$\n",
    "P\\left(y_{i}=1\\left|x\\right.\\right)=G\\left(x^{\\prime}\\beta\\right)=G\\left(x^{\\prime}\\beta\\right)=\\frac{\\exp\\left(x^{\\prime}\\beta\\right)}{1+\\exp\\left(x^{\\prime}\\beta\\right)} \\tag{2}\n",
    "$$\n",
    "The outcome in binary models is Bernoulli distributed (the binomial\n",
    "distribution with only one trial). The probability mass function for\n",
    "$y_{i}$ is,\n",
    "$$\n",
    "y_{i}=f\\left(y_{i}\\left|x_{i}\\right.\\right)=p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\\quad y_{i} \\in \\{0,1\\}\n",
    "$$\n",
    "with\n",
    "$p_{i}=G\\left(x_i^{\\prime}\\beta\\right)=\\frac{\\exp\\left(x_i^{\\prime}\\beta\\right)}{1+\\exp\\left(x_i^{\\prime}\\beta\\right)}$.\n",
    "The log-likelihood contribution for observation $i$ is,\n",
    "$$\n",
    "L\\left(\\beta\\right)=y_{i}\\log G\\left(x_{i}^{\\prime}\\beta\\right)+\\left(1-y_{i}\\right)\\log\\left(1-G\\left(x_{i}^{\\prime}\\beta\\right)\\right)\n",
    "$$\n",
    "And the log--likelihood function is, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_{N}\\left(\\beta\\right)=\\sum_{i=1}^{N}\\left\\{ y_{i}\\log G\\left(x_{i}^{\\prime}\\beta\\right)+\\left(1-y_{i}\\right)\\log\\left(1-G\\left(x_{i}^{\\prime}\\beta\\right)\\right)\\right\\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the empty parts in the logit_criterion() function. This takes some best guess beta parameters (in this case the beta parameters from a LPM is a good first guess), and the data values y and x. It returns a vector of likelihood \n",
    "\n",
    "# z should be the input of G, what does eq. (3) suggest that z is?\n",
    "# What does eq. (2) suggets that the functional form of G?\n",
    "# Finaly, write up the function for the likelihood contribution and return it\n",
    "\n",
    "# You can check with the cell below that you have written the logit_criterion correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "np.isclose(np.sum(nlm.logit_criterion(ols_results['b_hat'], y, x)), -606.5379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "estimate() missing 4 required positional arguments: 'func', 'theta0', 'y', and 'x'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1b051f1171f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m logit_result = nlm.estimate(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Fill in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: estimate() missing 4 required positional arguments: 'func', 'theta0', 'y', and 'x'"
     ]
    }
   ],
   "source": [
    "logit_result = nlm.estimate(\n",
    "    # Fill in\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.583469\n         Iterations: 45\n         Function evaluations: 441\n         Gradient evaluations: 49\n"
     ]
    }
   ],
   "source": [
    "logit_result = nlm.estimate(\n",
    "    nlm.logit_criterion, ols_results['b_hat'].flatten(), y.flatten(), x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logit results\nDependent variable: lfp\n\n           Beta     Se    t-values\n-------  ------  -----  ----------\nconst     6.196  2.482       2.496\nage       3.437  0.709       4.846\nagesq    -0.488  0.087      -5.592\neduc      0.033  0.030       1.082\nnyc      -1.186  0.165      -7.201\nnoc      -0.241  0.083      -2.894\nnlinc    -1.104  0.230      -4.792\nforeign   1.168  0.203       5.769\nIn 45 iterations and 441 function evaluations.\n"
     ]
    }
   ],
   "source": [
    "nlm.print_table(\n",
    "    (y_lab, x_lab), logit_result, \n",
    "    title='Logit results', floatfmt='.3f'\n",
    ")"
   ]
  },
  {
   "source": [
    "Your table should approx. look like this:\n",
    "\n",
    "Logit results <br>\n",
    "Dependent variable: lfp\n",
    "\n",
    "|         |   Beta |    Se |   t-values |\n",
    "|---------|--------|-------|------------|\n",
    "| const   |  6.196 | 2.482 |      2.496 |\n",
    "| age     |  3.437 | 0.709 |      4.846 |\n",
    "| agesq   | -0.488 | 0.087 |     -5.592 |\n",
    "| educ    |  0.033 | 0.030 |      1.082 |\n",
    "| nyc     | -1.186 | 0.165 |     -7.201 |\n",
    "| noc     | -0.241 | 0.083 |     -2.894 |\n",
    "| nlinc   | -1.104 | 0.230 |     -4.792 |\n",
    "| foreign |  1.168 | 0.203 |      5.769 |\n",
    "In 45 iterations and 441 function evaluations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 5\n",
    "Calculate the ratio between the beta coefficient between\n",
    "- $\\frac{\\hat{\\beta}_{Logit}}{\\hat{\\beta}_{OLS}}$\n",
    "- $\\frac{\\hat{\\beta}_{Probit}}{\\hat{\\beta}_{OLS}}$\n",
    "- $\\frac{\\hat{\\beta}_{Logit}}{\\hat{\\beta}_{Probit}}$\n",
    "\n",
    "The explanation for why the coefficients are not equal for the LPM, the\n",
    "Logit and the Probit model is that these three models use different link\n",
    "functions for the probabilities and, as mentioned by Cameron & Trivedi,\n",
    "it makes more sense to compare the marginal effects across the three\n",
    "models. More specifically, the location and scale are set differently in\n",
    "these models, leading to different parameter estimates. A rule of thumb\n",
    "is that, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}_{Logit}&\\simeq4\\hat{\\beta}_{OLS}\\\\\n",
    "\\hat{\\beta}_{Probit}&\\simeq2.5\\hat{\\beta}_{OLS}\\\\\n",
    "\\hat{\\beta}_{Logit}&\\simeq1.6\\hat{\\beta}_{Probit}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3.7244563  5.03506263 5.02590696 4.90716746 4.92771395 4.88624976\n  5.1873801  4.68091829]]\n[[2.2534614  3.04053118 3.0336372  2.88361919 2.96923174 2.98085622\n  3.13346871 2.86207081]]\n[1.65277128 1.65598125 1.65672644 1.70173908 1.65959224 1.63921015\n 1.65547531 1.63550052]\n"
     ]
    }
   ],
   "source": [
    "# Use the dictionaries from the three estimations to calculate the ratios.\n",
    "# The vectors might not line up, and you might have to transpose some of them.\n",
    "print(logit_result['b_hat']/ols_results['b_hat'].T)\n",
    "print(probit_result['b_hat']/ols_results['b_hat'].T)\n",
    "print(logit_result['b_hat']/probit_result['b_hat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Calculate the marginal effect of taking one additional year of\n",
    "education on the probability of participating in the labor market for a\n",
    "woman with the following characteristics (Remember that the variable $\\texttt{AGE}$ is divided by 10, so 2.5 does make sense):\n",
    "\n",
    "$\\texttt{AGE} $= 2.5,\n",
    "$\\texttt{EDUC}$ = 10,\n",
    "$\\texttt{NYC} $= 1,\n",
    "$\\texttt{NOC} $= 0,\n",
    "$\\texttt{NLINC}$ = 10,\n",
    "$\\texttt{FOREIGN}$ = 0. \n",
    "\n",
    "Consider education as a continuous variable. The marginal effect should be calculated for the LPM, the probit and the logit models."
   ]
  },
  {
   "source": [
    "The partial (also called marginal) effects in the Logit and Probit\n",
    "models depend upon the regressors, $x_{k}$. For continuous variables the\n",
    "partial effects are given as,\n",
    "$$\n",
    "\\frac{\\partial P\\left(y_{i}=1\\left|x_{i}\\right.\\right)}{\\partial x_{ik}}=\\frac{\\partial p_{i}}{\\partial x_{ik}}=\\frac{\\partial G\\left(x_{i}^{\\prime}\\beta\\right)}{\\partial x_{ik}^{\\prime}\\beta}\\cdot\\frac{\\partial x_{i}^{\\prime}\\beta}{\\partial x_{ik}^{\\prime}}=g\\left(x_{i}^{\\prime}\\beta\\right)\\beta_{k}\n",
    "$$\n",
    "where $g\\left(z\\right)=\\frac{\\partial G\\left(z\\right)}{\\partial z}$ and\n",
    "where\n",
    "$g\\left(z\\right)=\\frac{\\exp\\left(z\\right)}{\\left(1+\\exp\\left(z\\right)\\right)^{2}}$\n",
    "for the Logit model and\n",
    "$g\\left(z\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\frac{-z^{2}}{2}\\right)$\n",
    "for the Probit model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0066567]\n[[0.00762391]]\n[[0.0081144]]\n"
     ]
    }
   ],
   "source": [
    "# Let us make a vector of the values we want to investigate\n",
    "x_me = np.array([1.0, 2.5, 2.5**2, 10, 1, 0, 10, 0]).reshape(1, -1)\n",
    "\n",
    "# Let us get the beta coefficients that we are interested in.\n",
    "b_pr = probit_result.get('b_hat').reshape(-1, 1)\n",
    "b_lg = logit_result.get('b_hat').reshape(-1, 1)\n",
    "\n",
    "# Calculate the marginal effects bot for the logit and probit.\n",
    "# For the probit, you can use norm.pdf for g\n",
    "# For the logit, g should be straight forward using the given function.\n",
    "me_educ_pr = norm.pdf(x_me@b_pr)*b_pr[3]\n",
    "me_educ_lg = np.exp(x_me@b_lg)/(1 + np.exp(x_me@b_lg))**2*b_lg[3]\n",
    "\n",
    "print(ols_results['b_hat'][3])\n",
    "print(me_educ_pr)\n",
    "print(me_educ_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Calculate the marginal effect of being a permanent foreign resident on the probability of participating in the labor market.\n",
    "\n",
    "\n",
    "For discrete variables the partial effects are given as,\n",
    "$$\n",
    "G\\left(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{k-1}x_{k-1}+\\beta_{k}\\right)-G\\left(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{k-1}x_{k-1}\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$G\\left(x^{\\prime}\\beta\\right)=\\frac{\\exp\\left(x^{\\prime}\\beta\\right)}{1+\\exp\\left(x^{\\prime}\\beta\\right)}$\n",
    "\n",
    "for the Logit model and\n",
    "\n",
    "$G\\left(x^{\\prime}\\beta\\right)=\\Phi\\left(x^{\\prime}\\beta\\right)=\\int_{-\\infty}^{x^{\\prime}\\beta}\\phi\\left(z\\right)dz$\n",
    "\n",
    "and\n",
    "\n",
    "$\\phi\\left(z\\right)=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\exp\\left(\\frac{-z^{2}}{2}\\right)$\n",
    "\n",
    "for the Probit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.24960124]\n[[0.26995807]]\n[[0.27261031]]\n"
     ]
    }
   ],
   "source": [
    "# We will look at the same values as previously, but we want to look at the difference for foreign = 0 and foreign = 1.\n",
    "x_me2 = x_me.copy()\n",
    "x_me2[:, 7] = 1  # Keep everythin the same, but change foregin to 1\n",
    "\n",
    "# For the probit, calculate the norm.cdf for foreign = 1, and subtract foreign = 0\n",
    "me_foreign_pr = norm.cdf(x_me2@b_pr) - norm.cdf(x_me@b_pr) \n",
    "\n",
    "# The same for the logit, calculate using the G() function for logit, for foreign = 1, and subtract foreign = 0\n",
    "me_foreign_lg = (\n",
    "    np.exp(x_me2@b_lg)/(1 + np.exp(x_me2@b_lg)) -\n",
    "    np.exp(x_me@b_lg)/(1 + np.exp(x_me@b_lg))\n",
    ")\n",
    "\n",
    "print(ols_results['b_hat'][7])\n",
    "print(me_foreign_pr)\n",
    "print(me_foreign_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the standard errors of the marginal effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_c_pr = norm.pdf(x_me@b_pr)*(np.eye(k) - (b_pr@b_pr.T)@(x_me.T@x_me))\n",
    "grad_d_pr = norm.pdf(x_me2@b_pr)@x_me2 - norm.pdf(x_me@b_pr)@x_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_se(grad, cov):\n",
    "    return np.sqrt(np.diag(grad@cov@grad.T))\n",
    "\n",
    "se_c_pr = get_se(grad_c_pr, probit_result.get('cov'))\n",
    "se_d_pr = get_se(grad_d_pr, probit_result.get('cov'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Marginal effects, Probit\n\nVar        Coeff      se\n-------  -------  ------\neduc      0.0076  0.0072\nforeign   0.2700  0.0435\n"
     ]
    }
   ],
   "source": [
    "table = [\n",
    "    ['educ', me_educ_pr, se_c_pr[3]],\n",
    "    ['foreign', me_foreign_pr, se_d_pr]\n",
    "]\n",
    "print('Marginal effects, Probit')\n",
    "print()\n",
    "print(tabulate(table, ['Var', 'Coeff', 'se'], floatfmt='.4f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}